{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cab824ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CNN_param(cnn_day, TAIEX_df, Triple_day , hasTurning_point, TAIEX_bias_df , isBias ,threshold =0.65) :\n",
    "     \n",
    "    def training (): \n",
    "        day = cnn_day\n",
    "        df = TAIEX_df[::-1].reset_index(drop=True)\n",
    "        ret = fn.triple_barrier(TAIEX_df.Close, Triple_up ,Triple_down, Triple_day)\n",
    "\n",
    "        if isBias :        \n",
    "            dropLabel = ['STOCK_ID','Date']\n",
    "            #dropLabel = ['Volume','Dividends','Stock Splits','STOCK_ID','Date']\n",
    "        else :\n",
    "            dropLabel=['Date', 'Volume','Dividends','Stock Splits','STOCK_ID','Adj Close']\n",
    "\n",
    "        if hasTurning_point : \n",
    "            turning_date = fn.get_turning_point(df ,TAIEX_df, day)        \n",
    "            start = 0 \n",
    "            long_list = []\n",
    "            long_label = []\n",
    "            hasTurningPoint= []\n",
    "\n",
    "            for i in turning_date:\n",
    "                if start == 0 :\n",
    "                    start = i[0]\n",
    "                    continue\n",
    "                if isBias :\n",
    "                    split_data = TAIEX_bias_df[start+1 : i[0]+1].drop(dropLabel,axis=1)[::-1].reset_index(drop = True)\n",
    "                else :\n",
    "                    split_data = TAIEX_df[start+1 : i[0]+1].drop(dropLabel,axis=1)[::-1].reset_index(drop = True)\n",
    "                long_list.extend(fn.get_series_data(split_data , day , False, isBias))\n",
    "                long_label.extend(ret.triple_barrier_signal[start+day : i[0]+1][::-1])   \n",
    "                start = i[0]\n",
    "            long_list = np.array(long_list)\n",
    "        else :\n",
    "            if isBias : \n",
    "                df = TAIEX_bias_df[::-1].reset_index(drop=True)\n",
    "                price_df = df.drop(dropLabel,axis=1)\n",
    "            else :\n",
    "                price_df = df.drop(dropLabel,axis=1)\n",
    "            long_list = fn.get_series_data(price_df , day , False, isBias)\n",
    "            long_label = ret.triple_barrier_signal[day-1:len(ret)]\n",
    "\n",
    "        print(pd.Series(long_label).value_counts())\n",
    "        balance= max(pd.Series(long_label).value_counts())/len(long_label)\n",
    "\n",
    "    #     print(long_list , long_label)\n",
    "\n",
    "        ## CNN Training by split data\n",
    "        model, acc_list = cnn_training(long_list,long_label , day , 0.5 , 20 ,threshold)\n",
    "        \n",
    "        print(\"Param score: \", max(acc_list)/balance + max(acc_list)/2 )\n",
    "        \n",
    "        return max(acc_list)/balance + max(acc_list)/2\n",
    "    \n",
    "    Triple_down_list = [0.99,0.985,0.98,0.975,0.97,0.965,0.96]\n",
    "    Triple_up_list = [1.01,1.015,1.02,1.025,1.03,1.035,1.04,1.045,1.05]\n",
    "    Triple_up = Triple_up_list[math.ceil(Triple_day**0.7)] ;\n",
    "    \n",
    "    score_list = []\n",
    "    for Triple_down in Triple_down_list :        \n",
    "        score_list.append(training())\n",
    "    Triple_down = Triple_down_list[score_list.index(max(score_list))]\n",
    "    \n",
    "    score_list = []\n",
    "    for Triple_up in Triple_up_list :        \n",
    "        score_list.append(training())\n",
    "    Triple_up = Triple_up_list[score_list.index(max(score_list))]\n",
    "    \n",
    "    print(\"best down:\" ,Triple_down)\n",
    "    print(\"best up:\" ,Triple_up)\n",
    "    \n",
    "    return Triple_up, Triple_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d6506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CNN_cluster(cnn_day, TAIEX_df, Triple_up , Triple_down, Triple_day , num_cluster, hasTurning_point, TAIEX_bias_df , isBias ,threshold =0.45) :\n",
    "    day = cnn_day\n",
    "    df = TAIEX_df[::-1].reset_index(drop=True)\n",
    "    ret = fn.triple_barrier(TAIEX_df.Close, Triple_up ,Triple_down, Triple_day)\n",
    "    \n",
    "    if isBias :        \n",
    "        dropLabel = ['STOCK_ID','Date']\n",
    "        #dropLabel = ['Volume','Dividends','Stock Splits','STOCK_ID','Date']\n",
    "    else :\n",
    "        dropLabel=['Date', 'Volume','Dividends','Stock Splits','STOCK_ID','Adj Close']\n",
    "    \n",
    "    if hasTurning_point : \n",
    "        turning_date = fn.get_turning_point(df ,TAIEX_df, day)        \n",
    "        start = 0 \n",
    "        long_list = []\n",
    "        long_label = []\n",
    "        hasTurningPoint= []\n",
    "        \n",
    "        for i in turning_date:\n",
    "            if start == 0 :\n",
    "                start = i[0]\n",
    "                continue\n",
    "            if isBias :\n",
    "                split_data = TAIEX_bias_df[start+1 : i[0]+1].drop(dropLabel,axis=1)[::-1].reset_index(drop = True)\n",
    "            else :\n",
    "                split_data = TAIEX_df[start+1 : i[0]+1].drop(dropLabel,axis=1)[::-1].reset_index(drop = True)\n",
    "            long_list.extend(fn.get_series_data(split_data , day , False, isBias))\n",
    "            long_label.extend(ret.triple_barrier_signal[start+day : i[0]+1][::-1])   \n",
    "            start = i[0]\n",
    "        long_list = np.array(long_list)\n",
    "    else :\n",
    "        if isBias : \n",
    "            df = TAIEX_bias_df[::-1].reset_index(drop=True)\n",
    "            price_df = df.drop(dropLabel,axis=1)\n",
    "        else :\n",
    "            price_df = df.drop(dropLabel,axis=1)\n",
    "        long_list = fn.get_series_data(price_df , day , False, isBias)\n",
    "        long_label = ret.triple_barrier_signal[day-1:len(ret)]\n",
    "        \n",
    "    print(pd.Series(long_label).value_counts())\n",
    "    \n",
    "#     print(long_list , long_label)\n",
    "        \n",
    "    ## CNN Training by split data\n",
    "    model, acc_list = fn.cnn_training(long_list,long_label , day , 0.5 , 30,threshold)\n",
    "\n",
    "    if isBias : \n",
    "        df = TAIEX_bias_df[::-1].reset_index(drop=True)\n",
    "        price_df = df.drop(dropLabel,axis=1)\n",
    "    else :\n",
    "        price_df = df.drop(dropLabel,axis=1)\n",
    "    ## 取得CNN 最後一層的output\n",
    "    flatten_list = []\n",
    "\n",
    "    ## Ini all data\n",
    "    long_list = fn.get_series_data(price_df , day , False , isBias)\n",
    "    long_label = ret.triple_barrier_signal[day-1:len(ret)]\n",
    "    long_label = long_label[::-1].reset_index(drop=True)\n",
    "        \n",
    "    predict_x=model.predict(long_list) \n",
    "    cnn_predict=np.argmax(predict_x,axis=-1)\n",
    "    \n",
    "    for periodData in long_list :\n",
    "        keract_inputs = periodData.reshape(1 , long_list.shape[1], long_list.shape[2],1)\n",
    "        activations = get_activations(model, keract_inputs)\n",
    "        flatten_list.append(activations['Dense'])\n",
    "    long_cluster_label = pd.Series(fn.get_cluster(flatten_list, num_cluster))\n",
    "    print(long_cluster_label.value_counts())\n",
    "    \n",
    "    if isBias : \n",
    "        return long_cluster_label , long_label\n",
    "    else :\n",
    "        return long_cluster_label , long_label , cnn_predict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891b008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cnn_training(allData, allLabel, day , splitsize, epoches = 100 , threshold = 0.45) :\n",
    "    score = 0\n",
    "    acc_list = []\n",
    "    iter = 0\n",
    "    while score < threshold and iter < 5:          \n",
    "        week_list = allData\n",
    "        week_label = allLabel\n",
    "        # 定義梯度下降批量\n",
    "        batch_size = 32\n",
    "        # 定義分類數量\n",
    "        num_classes = 3\n",
    "        # 定義訓練週期\n",
    "        epochs = epoches\n",
    "\n",
    "        # 定義圖像寬、高\n",
    "        img_rows, img_cols = day, 4\n",
    "        input_shape = ( img_rows, img_cols)\n",
    "\n",
    "        # 載入 MNIST 訓練資料\n",
    "        split_ratio = splitsize\n",
    "        x_train = week_list[ math.ceil(len(week_list)*split_ratio) :]\n",
    "        x_test = week_list[ math.ceil(len(week_list)*split_ratio*0.4) : math.ceil(len(week_list)*split_ratio) ]\n",
    "\n",
    "        y_train = week_label[ math.ceil(len(week_label)*split_ratio) :]\n",
    "        y_test = week_label[ math.ceil(len(week_list)*split_ratio*0.4)  : math.ceil(len(week_label)*split_ratio) ]\n",
    "\n",
    "        x_train = x_train.reshape(x_train.shape[0] , img_rows, img_cols,1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols ,1)\n",
    "\n",
    "        # x_train  = torch.from_numpy(x_train)\n",
    "        # x_test  = torch.from_numpy(x_test)\n",
    "\n",
    "        # y_train = torch.from_numpy(y_train)\n",
    "        # y_test = torch.from_numpy(y_test)\n",
    "\n",
    "        input_shape = (img_rows, img_cols,1 )\n",
    "\n",
    "        # 保留原始資料，供 cross tab function 使用\n",
    "        y_test_org = y_test\n",
    "\n",
    "\n",
    "        # y 值轉成 one-hot encoding\n",
    "        y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "        y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "        # 建立簡單的線性執行的模型\n",
    "        model = Sequential()\n",
    "        # 建立卷積層，filter=32,即 output space 的深度, Kernal Size: 3x3, activation function 採用 relu\n",
    "        model.add(Conv2D(16, kernel_size=(3,2),\n",
    "                        activation='relu',\n",
    "                        input_shape=input_shape))\n",
    "        # 建立池化層，池化大小=2x2，取最大值\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(GaussianNoise(0.3))\n",
    "        # 建立卷積層，filter=64,即 output size, Kernal Size: 3x3, activation function 採用 relu\n",
    "        model.add(Conv2D(32,  kernel_size=(3,2), activation='relu'))\n",
    "        # 建立池化層，池化大小=2x2，取最大值\n",
    "        model.add(AvgPool2D(pool_size=(3, 2)))\n",
    "        # Dropout層隨機斷開輸入神經元，用於防止過度擬合，斷開比例:0.25\n",
    "        model.add(Dropout(0.25))\n",
    "        # Flatten層把多維的輸入一維化，常用在從卷積層到全連接層的過渡。\n",
    "        model.add(Flatten( name ='flatten'))\n",
    "        # 全連接層: 128個output\n",
    "        model.add(Dense(batch_size, 'sigmoid', name ='Dense'))\n",
    "        # 使用 softmax activation function，將結果分類\n",
    "        model.add(Dense(num_classes, activation='softmax' ))\n",
    "\n",
    "        # 編譯: 選擇損失函數、優化方法及成效衡量方式\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "        # 進行訓練, 訓練過程會存在 train_history 變數中\n",
    "        train_history = model.fit(x_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=0,\n",
    "                validation_data=(x_test, y_test))\n",
    "\n",
    "        # 顯示損失函數、訓練成果(分數)\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print('Test loss:', score)\n",
    "        print('Test accuracy:', score)\n",
    "        acc_list.append(score[1])\n",
    "        iter +=1\n",
    "        score = score[1]\n",
    "    return model, acc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8fd48e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import calendar\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "import math\n",
    "import cnn_feature_handler as fn\n",
    "from keract import get_activations, display_activations\n",
    "\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import calendar\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "import math\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Conv1D,Conv2D, MaxPooling2D , AvgPool2D,AvgPool1D ,SeparableConv2D\n",
    "import tensorflow as tf \n",
    "from keras.layers import GaussianNoise , BatchNormalization\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import metrics\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a0d525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "標的 :  2379\n"
     ]
    }
   ],
   "source": [
    "9## read all data\n",
    "path = \"./Data/\" \n",
    "allData = pd.read_csv(path+'Stock.csv')\n",
    "allIndex = pd.read_csv(path+'noTrend_model.csv')\n",
    "columns = ['Date','Open','High','Low','Close','Volume'] \n",
    "#TAIEX_df.columns = columns\n",
    "stock_list = pd.Series(allData.STOCK_ID, dtype=\"category\").cat.categories.tolist()\n",
    "\n",
    "index = 9\n",
    "print(\"標的 : \",stock_list[index])\n",
    "TAIEX_df = allData[allData.STOCK_ID == stock_list[index]].reset_index(drop=True)[10:].reset_index(drop=True)\n",
    "TAIEX_Index = allIndex[allIndex.STOCK_ID == stock_list[index]].reset_index(drop=True)[10:].reset_index(drop=True)\n",
    "df = TAIEX_df[::-1].reset_index(drop=True)\n",
    "dropLabel=['Date', 'Volume','Dividends','Stock Splits','STOCK_ID','Adj Close']\n",
    "price_df = df.drop(dropLabel,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a626a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1216,\n",
       " 1301,\n",
       " 1303,\n",
       " 1326,\n",
       " 2002,\n",
       " 2303,\n",
       " 2308,\n",
       " 2317,\n",
       " 2330,\n",
       " 2379,\n",
       " 2412,\n",
       " 2454,\n",
       " 2603,\n",
       " 2881,\n",
       " 2882,\n",
       " 2884,\n",
       " 2885,\n",
       " 2886,\n",
       " 2891,\n",
       " 3034,\n",
       " 3711,\n",
       " 5871,\n",
       " 6415]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20890e60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#long_cluster , long_label, long_out = get_CNN_cluster(20, TAIEX_df, 1.05, 0.96, 10, 3 , True, [], False,0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e97a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    1884\n",
      " 1    1186\n",
      " 0     324\n",
      "dtype: int64\n",
      "Test loss: [0.8998963236808777, 0.5343811511993408]\n",
      "Test accuracy: [0.8998963236808777, 0.5343811511993408]\n",
      "Test loss: [0.9083754420280457, 0.5442042946815491]\n",
      "Test accuracy: [0.9083754420280457, 0.5442042946815491]\n",
      "Test loss: [0.9055171608924866, 0.5383104085922241]\n",
      "Test accuracy: [0.9055171608924866, 0.5383104085922241]\n",
      "Test loss: [0.8995985984802246, 0.5422396659851074]\n",
      "Test accuracy: [0.8995985984802246, 0.5422396659851074]\n",
      "Test loss: [0.9109885692596436, 0.5294695496559143]\n",
      "Test accuracy: [0.9109885692596436, 0.5294695496559143]\n",
      "balance score:  0.9803765266184594\n",
      "-1    1678\n",
      " 1    1266\n",
      " 0     450\n",
      "dtype: int64\n",
      "Test loss: [0.9746862649917603, 0.48624753952026367]\n",
      "Test accuracy: [0.9746862649917603, 0.48624753952026367]\n",
      "Test loss: [0.9811578392982483, 0.4833005964756012]\n",
      "Test accuracy: [0.9811578392982483, 0.4833005964756012]\n",
      "Test loss: [0.9676364660263062, 0.4882121682167053]\n",
      "Test accuracy: [0.9676364660263062, 0.4882121682167053]\n",
      "Test loss: [0.968222975730896, 0.48624753952026367]\n",
      "Test accuracy: [0.968222975730896, 0.48624753952026367]\n",
      "Test loss: [0.9741385579109192, 0.4882121682167053]\n",
      "Test accuracy: [0.9741385579109192, 0.4882121682167053]\n",
      "balance score:  0.9874803926862323\n",
      "-1    1474\n",
      " 1    1327\n",
      " 0     593\n",
      "dtype: int64\n",
      "Test loss: [1.0353686809539795, 0.4489194452762604]\n",
      "Test accuracy: [1.0353686809539795, 0.4489194452762604]\n",
      "Test loss: [1.0112979412078857, 0.43909627199172974]\n",
      "Test accuracy: [1.0112979412078857, 0.43909627199172974]\n",
      "Test loss: [1.0215603113174438, 0.43123772740364075]\n",
      "Test accuracy: [1.0215603113174438, 0.43123772740364075]\n",
      "Test loss: [1.0216933488845825, 0.4538310468196869]\n",
      "Test accuracy: [1.0216933488845825, 0.4538310468196869]\n",
      "Test loss: [1.0146814584732056, 0.44302552938461304]\n",
      "Test accuracy: [1.0146814584732056, 0.44302552938461304]\n",
      "balance score:  1.0449813927449234\n",
      " 1    1375\n",
      "-1    1309\n",
      " 0     710\n",
      "dtype: int64\n",
      "Test loss: [1.064058542251587, 0.4076620936393738]\n",
      "Test accuracy: [1.064058542251587, 0.4076620936393738]\n",
      "Test loss: [1.0429787635803223, 0.40176817774772644]\n",
      "Test accuracy: [1.0429787635803223, 0.40176817774772644]\n",
      "Test loss: [1.0399621725082397, 0.4292730987071991]\n",
      "Test accuracy: [1.0399621725082397, 0.4292730987071991]\n",
      "Test loss: [1.044494867324829, 0.43123772740364075]\n",
      "Test accuracy: [1.044494867324829, 0.43123772740364075]\n",
      "Test loss: [1.0360016822814941, 0.4459725022315979]\n",
      "Test accuracy: [1.0360016822814941, 0.4459725022315979]\n",
      "balance score:  1.100822307326577\n",
      " 1    1402\n",
      "-1    1151\n",
      " 0     841\n",
      "dtype: int64\n",
      "Test loss: [1.0788720846176147, 0.4174852669239044]\n",
      "Test accuracy: [1.0788720846176147, 0.4174852669239044]\n",
      "Test loss: [1.0642911195755005, 0.4292730987071991]\n",
      "Test accuracy: [1.0642911195755005, 0.4292730987071991]\n",
      "Test loss: [1.0807840824127197, 0.40275049209594727]\n",
      "Test accuracy: [1.0807840824127197, 0.40275049209594727]\n",
      "Test loss: [1.0939180850982666, 0.41846758127212524]\n",
      "Test accuracy: [1.0939180850982666, 0.41846758127212524]\n",
      "Test loss: [1.0680503845214844, 0.42632612586021423]\n",
      "Test accuracy: [1.0680503845214844, 0.42632612586021423]\n",
      "balance score:  1.039196074901736\n",
      " 1    1425\n",
      "-1    1017\n",
      " 0     952\n",
      "dtype: int64\n",
      "Test loss: [1.0801491737365723, 0.41453832387924194]\n",
      "Test accuracy: [1.0801491737365723, 0.41453832387924194]\n",
      "Test loss: [1.1092400550842285, 0.41846758127212524]\n",
      "Test accuracy: [1.1092400550842285, 0.41846758127212524]\n",
      "Test loss: [1.0841959714889526, 0.4204322099685669]\n",
      "Test accuracy: [1.0841959714889526, 0.4204322099685669]\n",
      "Test loss: [1.0931143760681152, 0.4115913510322571]\n",
      "Test accuracy: [1.0931143760681152, 0.4115913510322571]\n",
      "Test loss: [1.0834177732467651, 0.41453832387924194]\n",
      "Test accuracy: [1.0834177732467651, 0.41453832387924194]\n",
      "balance score:  1.0013662600935551\n",
      " 1    1443\n",
      " 0    1044\n",
      "-1     907\n",
      "dtype: int64\n",
      "Test loss: [1.0892025232315063, 0.41846758127212524]\n",
      "Test accuracy: [1.0892025232315063, 0.41846758127212524]\n",
      "Test loss: [1.0924478769302368, 0.4174852669239044]\n",
      "Test accuracy: [1.0924478769302368, 0.4174852669239044]\n",
      "Test loss: [1.1054975986480713, 0.37622788548469543]\n",
      "Test accuracy: [1.1054975986480713, 0.37622788548469543]\n",
      "Test loss: [1.0819106101989746, 0.40569743514060974]\n",
      "Test accuracy: [1.0819106101989746, 0.40569743514060974]\n",
      "Test loss: [1.116190791130066, 0.4165029525756836]\n",
      "Test accuracy: [1.116190791130066, 0.4165029525756836]\n",
      "balance score:  0.984254311044763\n",
      " 1    2051\n",
      "-1    1124\n",
      " 0     219\n",
      "dtype: int64\n",
      "Test loss: [0.8522887825965881, 0.5923379063606262]\n",
      "Test accuracy: [0.8522887825965881, 0.5923379063606262]\n",
      "Test loss: [0.8542629480361938, 0.5913556218147278]\n",
      "Test accuracy: [0.8542629480361938, 0.5913556218147278]\n",
      "Test loss: [0.8511959910392761, 0.5923379063606262]\n",
      "Test accuracy: [0.8511959910392761, 0.5923379063606262]\n",
      "Test loss: [0.8698195815086365, 0.5923379063606262]\n",
      "Test accuracy: [0.8698195815086365, 0.5923379063606262]\n",
      "Test loss: [0.8872002363204956, 0.5923379063606262]\n",
      "Test accuracy: [0.8872002363204956, 0.5923379063606262]\n",
      "balance score:  0.9802022692286522\n",
      " 1    1864\n",
      "-1    1197\n",
      " 0     333\n",
      "dtype: int64\n",
      "Test loss: [0.9308216571807861, 0.5383104085922241]\n",
      "Test accuracy: [0.9308216571807861, 0.5383104085922241]\n",
      "Test loss: [0.9360929131507874, 0.5353634357452393]\n",
      "Test accuracy: [0.9360929131507874, 0.5353634357452393]\n",
      "Test loss: [0.9237666130065918, 0.531434178352356]\n",
      "Test accuracy: [0.9237666130065918, 0.531434178352356]\n",
      "Test loss: [0.923321008682251, 0.5275049209594727]\n",
      "Test accuracy: [0.923321008682251, 0.5275049209594727]\n",
      "Test loss: [0.921257495880127, 0.5432220101356506]\n",
      "Test accuracy: [0.921257495880127, 0.5432220101356506]\n",
      "balance score:  0.989107029184763\n",
      " 1    1680\n",
      "-1    1259\n",
      " 0     455\n",
      "dtype: int64\n",
      "Test loss: [0.971951961517334, 0.505893886089325]\n",
      "Test accuracy: [0.971951961517334, 0.505893886089325]\n",
      "Test loss: [0.9774134755134583, 0.5078585743904114]\n",
      "Test accuracy: [0.9774134755134583, 0.5078585743904114]\n",
      "Test loss: [0.9724206328392029, 0.4970530569553375]\n",
      "Test accuracy: [0.9724206328392029, 0.4970530569553375]\n",
      "Test loss: [0.9759442806243896, 0.4970530569553375]\n",
      "Test accuracy: [0.9759442806243896, 0.4970530569553375]\n",
      "Test loss: [0.9808762073516846, 0.5088408589363098]\n",
      "Test accuracy: [0.9808762073516846, 0.5088408589363098]\n",
      "balance score:  1.0279796876368068\n",
      " 1    1524\n",
      "-1    1288\n",
      " 0     582\n",
      "dtype: int64\n",
      "Test loss: [1.0233104228973389, 0.4548133611679077]\n",
      "Test accuracy: [1.0233104228973389, 0.4548133611679077]\n",
      "Test loss: [0.9954071640968323, 0.48526522517204285]\n",
      "Test accuracy: [0.9954071640968323, 0.48526522517204285]\n",
      "Test loss: [1.0060609579086304, 0.4361492991447449]\n",
      "Test accuracy: [1.0060609579086304, 0.4361492991447449]\n",
      "Test loss: [1.007527232170105, 0.46463653445243835]\n",
      "Test accuracy: [1.007527232170105, 0.46463653445243835]\n",
      "Test loss: [1.0015475749969482, 0.4882121682167053]\n",
      "Test accuracy: [1.0015475749969482, 0.4882121682167053]\n",
      "balance score:  1.0872651567765734\n",
      " 1    1375\n",
      "-1    1309\n",
      " 0     710\n",
      "dtype: int64\n",
      "Test loss: [1.0564099550247192, 0.40962672233581543]\n",
      "Test accuracy: [1.0564099550247192, 0.40962672233581543]\n",
      "Test loss: [1.0512181520462036, 0.45284873247146606]\n",
      "Test accuracy: [1.0512181520462036, 0.45284873247146606]\n",
      "Test loss: [1.034226655960083, 0.46954813599586487]\n",
      "Test accuracy: [1.034226655960083, 0.46954813599586487]\n",
      "Test loss: [1.0591458082199097, 0.42239686846733093]\n",
      "Test accuracy: [1.0591458082199097, 0.42239686846733093]\n",
      "Test loss: [1.0435137748718262, 0.41453832387924194]\n",
      "Test accuracy: [1.0435137748718262, 0.41453832387924194]\n",
      "balance score:  1.1590155444145203\n",
      "-1    1333\n",
      " 1    1232\n",
      " 0     829\n",
      "dtype: int64\n",
      "Test loss: [1.086135983467102, 0.3614931106567383]\n",
      "Test accuracy: [1.086135983467102, 0.3614931106567383]\n",
      "Test loss: [1.0738918781280518, 0.3801571726799011]\n",
      "Test accuracy: [1.0738918781280518, 0.3801571726799011]\n",
      "Test loss: [1.0691068172454834, 0.40176817774772644]\n",
      "Test accuracy: [1.0691068172454834, 0.40176817774772644]\n",
      "Test loss: [1.0879112482070923, 0.38899803161621094]\n",
      "Test accuracy: [1.0879112482070923, 0.38899803161621094]\n",
      "Test loss: [1.0716500282287598, 0.3948919475078583]\n",
      "Test accuracy: [1.0716500282287598, 0.3948919475078583]\n",
      "balance score:  1.0229566356157416\n",
      "-1    1351\n",
      " 1    1105\n",
      " 0     938\n",
      "dtype: int64\n",
      "Test loss: [1.0904982089996338, 0.41944989562034607]\n",
      "Test accuracy: [1.0904982089996338, 0.41944989562034607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: [1.0987738370895386, 0.33889979124069214]\n",
      "Test accuracy: [1.0987738370895386, 0.33889979124069214]\n",
      "Test loss: [1.0850929021835327, 0.3958742618560791]\n",
      "Test accuracy: [1.0850929021835327, 0.3958742618560791]\n",
      "Test loss: [1.0828115940093994, 0.3988212049007416]\n",
      "Test accuracy: [1.0828115940093994, 0.3988212049007416]\n",
      "Test loss: [1.08767831325531, 0.3752455711364746]\n",
      "Test accuracy: [1.08767831325531, 0.3752455711364746]\n",
      "balance score:  1.0537475542083306\n",
      "-1    1366\n",
      " 0    1038\n",
      " 1     990\n",
      "dtype: int64\n",
      "Test loss: [1.0997203588485718, 0.36051079630851746]\n",
      "Test accuracy: [1.0997203588485718, 0.36051079630851746]\n",
      "Test loss: [1.105703592300415, 0.34675833582878113]\n",
      "Test accuracy: [1.105703592300415, 0.34675833582878113]\n",
      "Test loss: [1.1019619703292847, 0.3998035490512848]\n",
      "Test accuracy: [1.1019619703292847, 0.3998035490512848]\n",
      "Test loss: [1.0894840955734253, 0.40962672233581543]\n",
      "Test accuracy: [1.0894840955734253, 0.40962672233581543]\n",
      "Test loss: [1.1002439260482788, 0.3634577691555023]\n",
      "Test accuracy: [1.1002439260482788, 0.3634577691555023]\n",
      "balance score:  1.017769469698212\n",
      "-1    1377\n",
      " 0    1135\n",
      " 1     882\n",
      "dtype: int64\n",
      "Test loss: [1.0920661687850952, 0.38506877422332764]\n",
      "Test accuracy: [1.0920661687850952, 0.38506877422332764]\n",
      "Test loss: [1.0944817066192627, 0.3958742618560791]\n",
      "Test accuracy: [1.0944817066192627, 0.3958742618560791]\n",
      "Test loss: [1.122896671295166, 0.36051079630851746]\n",
      "Test accuracy: [1.122896671295166, 0.36051079630851746]\n",
      "Test loss: [1.100557804107666, 0.4007858633995056]\n",
      "Test accuracy: [1.100557804107666, 0.4007858633995056]\n",
      "Test loss: [1.1109778881072998, 0.37721022963523865]\n",
      "Test accuracy: [1.1109778881072998, 0.37721022963523865]\n",
      "balance score:  0.9878483808118533\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8580/3599992022.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mshort_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mTriple_up\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTriple_down\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_CNN_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTAIEX_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshort_day\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8580/1247639557.py\u001b[0m in \u001b[0;36mget_CNN_param\u001b[1;34m(cnn_day, TAIEX_df, Triple_day, hasTurning_point, TAIEX_bias_df, isBias, threshold)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mTriple_up\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTriple_up_list\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mscore_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mTriple_up\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTriple_up_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best down:\"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mTriple_down\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "short_day = 5\n",
    "Triple_up,Triple_down = get_CNN_param(8, TAIEX_df, short_day , True, [], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e56247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "short_cluster, short_label ,short_out= get_CNN_cluster(8, TAIEX_df, Triple_up, Triple_down, short_day , 4 , True, [], False,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb6d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_cluster , long_label, long_out = get_CNN_cluster(16, TAIEX_df, (Triple_up*2-1), (Triple_down*2-1), short_day*2, 3 , True, [], False,0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = False\n",
    "if bias : \n",
    "    allBiasData = pd.read_csv(path+'Windows5_test.csv')\n",
    "    TAIEX_bias_df = allBiasData[allBiasData.STOCK_ID == stock_list[index]]\n",
    "    TAIEX_bias_df = TAIEX_bias_df[TAIEX_bias_df.Date >= TAIEX_df.Date.min()]\n",
    "    TAIEX_bias_df = TAIEX_bias_df[TAIEX_bias_df.Date <= TAIEX_df.Date.max()].reset_index(drop=True)\n",
    "\n",
    "    # df = TAIEX_bias_df[::-1].reset_index(drop=True)\n",
    "    # dropLabel = ['Volume','Dividends','Stock Splits','STOCK_ID','Date']\n",
    "    # price_df = df.drop(dropLabel,axis=1)\n",
    "\n",
    "    long_bias_cluster , long_bias_label = get_CNN_cluster(20, TAIEX_df, 1.05, 0.97, 10, 4 , False, TAIEX_bias_df, True)\n",
    "    short_bias_cluster, short_bias_label = get_CNN_cluster(8, TAIEX_df, 1.03, 0.98, 5 , 6 , False, TAIEX_bias_df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763414d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (len(short_cluster), short_cluster.value_counts())\n",
    "short_cluster = short_cluster[:len(long_cluster)]\n",
    "short_out = short_out[:len(long_cluster)]\n",
    "if bias: \n",
    "    short_bias_cluster = short_bias_cluster[:len(long_cluster)]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b438f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAIEX_Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15eeefd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read index\n",
    "\n",
    "Y = pd.DataFrame()\n",
    "Y['long'] = long_label.reset_index(drop = True)\n",
    "Y['short'] = short_label[:len(long_label)]\n",
    "\n",
    "X = TAIEX_Index[::-1].reset_index(drop=True)[:len(short_cluster)]\n",
    "X['date'] = X.Date\n",
    "X = X.drop(['Open','High','Low','Close','Volume','STOCK_ID','Date','MA5', 'MA10',  'MA20','MA60', 'MA120',\n",
    "            'MA_cross_5_10','MA_cross_5_20','MA_cross_5_60','MA_cross_10_60','MA_cross_10_20','MA_cross_20_60','MA_cross_5_120'],axis = 1)\n",
    "#'pre5p0.1','pre5d0.1','pre3p0.05','pre3d0.05','pre1p0.03','pre1d0.03''開盤價', '最高價', '最低價', '收盤價', '成交股數', '證券代號',  '年前高', '季前高', '漲跌',\n",
    "#        '均漲天數', '均跌天數','MA60', 'RSI', 'MACD_signal','MACD','MA5', 'MA10',  'MA20', 'buy5', 'buy10',  'K', 'D',\n",
    "#         '實紅棒', '紅棒天線', '實黑棒', '黑棒天線', '上避雷針', '下避雷針', '連漲跌',\n",
    "#        'MACD_histogram', '日振幅', '波動率', 'MA交叉型態', 'KD交叉型態', 'KD交叉型態2',\n",
    "#        'MA5 slope', 'MA10 slope', 'MA60 slope', 'buy5 slope', 'buy10 slope',\n",
    "#        'MA5 acc', 'RSI5鈍化', 'D5', 'D60', 'D_upper_day', 'UD5', 'UD60',\n",
    "#        'UD前高壓力', 'UD波段壓立', '均線糾結1', '均線糾結2', '三陽開泰', '突破前高', '超漲跌','MACD_histogram', '日振幅', '波動率'\n",
    "# adding cluster\n",
    "X['long_cluster'] = long_cluster\n",
    "X['short_cluster'] = short_cluster\n",
    "\n",
    "# X['long_out'] = long_out\n",
    "# # X['short_out'] = short_out\n",
    "# X['long_bias_cluster'] = long_bias_cluster\n",
    "# X['short_bias_cluster'] = short_bias_cluster\n",
    "\n",
    "if bias:\n",
    "    X['long_bias_cluster'] = long_bias_cluster\n",
    "    X['short_bias_cluster'] = short_bias_cluster\n",
    "\n",
    "Y = Y[~X.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "X = X[~X.isin([np.nan, np.inf, -np.inf]).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1730051f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MACD_cross_6</th>\n",
       "      <th>overBuyOrSold(80/20)_6</th>\n",
       "      <th>cross_6</th>\n",
       "      <th>over_3days_6</th>\n",
       "      <th>MACD_cross_9</th>\n",
       "      <th>overBuyOrSold(80/20)_9</th>\n",
       "      <th>cross_9</th>\n",
       "      <th>over_3days_9</th>\n",
       "      <th>MACD_cross_12</th>\n",
       "      <th>overBuyOrSold(80/20)_12</th>\n",
       "      <th>cross_12</th>\n",
       "      <th>over_3days_12</th>\n",
       "      <th>date</th>\n",
       "      <th>long_cluster</th>\n",
       "      <th>short_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20220111</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20220110</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20220107</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20220106</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20220105</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000218</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000217</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000216</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000215</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5431</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000214</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5432 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MACD_cross_6  overBuyOrSold(80/20)_6  cross_6  over_3days_6  \\\n",
       "0              0.0                    -1.0      0.0           0.0   \n",
       "1              0.0                     0.0      0.0           0.0   \n",
       "2              0.0                     0.0      0.0           0.0   \n",
       "3              0.0                     0.0      0.0           0.0   \n",
       "4              0.0                     0.0      0.0           0.0   \n",
       "...            ...                     ...      ...           ...   \n",
       "5427           0.0                     0.0      0.0           0.0   \n",
       "5428           0.0                     0.0      1.0           0.0   \n",
       "5429           0.0                     0.0      0.0           0.0   \n",
       "5430           0.0                     0.0      0.0           0.0   \n",
       "5431           0.0                     0.0      0.0           0.0   \n",
       "\n",
       "      MACD_cross_9  overBuyOrSold(80/20)_9  cross_9  over_3days_9  \\\n",
       "0              0.0                    -1.0      0.0           0.0   \n",
       "1              0.0                     0.0      0.0           0.0   \n",
       "2              0.0                     0.0      0.0           0.0   \n",
       "3              0.0                     0.0      0.0           0.0   \n",
       "4              0.0                     0.0      0.0           0.0   \n",
       "...            ...                     ...      ...           ...   \n",
       "5427           0.0                     0.0      0.0           0.0   \n",
       "5428           0.0                     0.0      0.0           0.0   \n",
       "5429           0.0                     0.0      0.0           0.0   \n",
       "5430           0.0                     0.0      1.0           0.0   \n",
       "5431           0.0                     0.0     -1.0           0.0   \n",
       "\n",
       "      MACD_cross_12  overBuyOrSold(80/20)_12  cross_12  over_3days_12  \\\n",
       "0               0.0                     -1.0       0.0            0.0   \n",
       "1               0.0                      0.0       0.0            0.0   \n",
       "2               0.0                      0.0       0.0            0.0   \n",
       "3               0.0                      0.0       0.0            0.0   \n",
       "4               0.0                      0.0       0.0            0.0   \n",
       "...             ...                      ...       ...            ...   \n",
       "5427            0.0                      0.0       0.0            0.0   \n",
       "5428            0.0                      0.0       0.0            0.0   \n",
       "5429            0.0                      0.0       1.0            0.0   \n",
       "5430            0.0                      0.0       0.0            0.0   \n",
       "5431            0.0                      0.0       0.0            0.0   \n",
       "\n",
       "          date  long_cluster  short_cluster  \n",
       "0     20220111             1              0  \n",
       "1     20220110             1              2  \n",
       "2     20220107             1              2  \n",
       "3     20220106             1              2  \n",
       "4     20220105             1              2  \n",
       "...        ...           ...            ...  \n",
       "5427  20000218             2              1  \n",
       "5428  20000217             2              1  \n",
       "5429  20000216             2              1  \n",
       "5430  20000215             1              0  \n",
       "5431  20000214             1              2  \n",
       "\n",
       "[5432 rows x 15 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb3298",
   "metadata": {},
   "source": [
    "## training feature : train with all feature\n",
    "- serveal index and time serises price data\n",
    "---\n",
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb10d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score, precision_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e4b96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffcc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bec6247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short cluster :  0\n",
      " 1    711\n",
      "-1    532\n",
      " 0    303\n",
      "Name: short, dtype: int64\n",
      "short cluster :  1\n",
      " 1    492\n",
      "-1    486\n",
      " 0    245\n",
      "Name: short, dtype: int64\n",
      "short cluster :  2\n",
      " 1    565\n",
      "-1    473\n",
      " 0    254\n",
      "Name: short, dtype: int64\n",
      "short cluster :  3\n",
      "-1    572\n",
      " 1    533\n",
      " 0    266\n",
      "Name: short, dtype: int64\n",
      "long cluster :  0\n",
      " 1    866\n",
      "-1    787\n",
      " 0    389\n",
      "Name: short, dtype: int64\n",
      "long cluster :  1\n",
      " 1    940\n",
      "-1    727\n",
      " 0    433\n",
      "Name: short, dtype: int64\n",
      "long cluster :  2\n",
      "-1    549\n",
      " 1    495\n",
      " 0    246\n",
      "Name: short, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "s_cluster = pd.Series(X.short_cluster, dtype=\"category\").cat.categories.tolist()\n",
    "for c in s_cluster : \n",
    "    print( \"short cluster : \", c)\n",
    "    print ( Y[X.short_cluster == c ].short.value_counts())\n",
    "        \n",
    "l_cluster = pd.Series(X.long_cluster, dtype=\"category\").cat.categories.tolist()\n",
    "for c in l_cluster : \n",
    "    print( \"long cluster : \", c)\n",
    "    print ( Y[X.long_cluster == c ].short.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d8b1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y[X.short_cluster.isin([0])].reset_index(drop=True)\n",
    "X = X[X.short_cluster.isin([0])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "695ee538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = Y[X.long_cluster.isin([0])].reset_index(drop=True)\n",
    "# X = X[X.long_cluster.isin([0])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a0436693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label :  long\n",
      "test date start form  20220111 to 20150401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1: [0.27306273 0.28571429 0.57526882]\n",
      "recall score(-1, 0, 1: [0.62711864 0.02020202 0.43319838]\n",
      "training score : 0.5341959334565619\n",
      "testing score : 0.39439655172413796\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label :  short\n",
      "test date start form  20220111 to 20150401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1: [0.32824427 0.30769231 0.5875    ]\n",
      "recall score(-1, 0, 1: [0.36134454 0.04761905 0.72030651]\n",
      "training score : 0.5101663585951941\n",
      "testing score : 0.5064655172413793\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_ = ['long' , 'short']\n",
    "predict_ = pd.DataFrame()\n",
    "predict_test = pd.DataFrame()\n",
    "isFirst = True\n",
    "for label in label_ :\n",
    "    test_ratio = 0.3\n",
    "    trainp = X [math.ceil(len(X)*test_ratio) :].reset_index(drop = True)\n",
    "    testp = X [ : math.ceil(len(X)*test_ratio)]\n",
    "    print(\"label : \", label )\n",
    "    print(\"test date start form \", testp.date[0],\"to\", testp.date[len(testp)-1] )\n",
    "\n",
    "    X_train = trainp.drop(['date'],axis = 1)\n",
    "    y_train = Y[label][math.ceil(len(X)*test_ratio) :]\n",
    "\n",
    "    X_test = testp.drop(['date'],axis = 1)\n",
    "    y_test = Y[label] [ : math.ceil(len(X)*test_ratio)]\n",
    "\n",
    "    #X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "    #ros = RandomOverSampler(random_state = 40)\n",
    "\n",
    "    #X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    xgbc = XGBClassifier( booster='gbtree', max_depth=5,                \n",
    "                  n_estimators=200, n_jobs=4, nthread=-1, eval_metric='mlogloss',\n",
    "                  random_state=27,  tree_method='exact',\n",
    "                  validate_parameters=1, verbosity=None)\n",
    "    \n",
    "    xgbc.fit(X_train,y_train)\n",
    "    y_test_pred = xgbc.predict(X_test)\n",
    "    y_train_pred = xgbc.predict(X_train)\n",
    "    precision, recall, f1, _ = score(y_test, y_test_pred)\n",
    "    \n",
    "#     print (max(xgbc.predict_proba(X_test).T[0]), min(xgbc.predict_proba(X_test).T[0]))\n",
    "#     print (max(xgbc.predict_proba(X_test).T[1]), min(xgbc.predict_proba(X_test).T[1]))\n",
    "#     test = pd.DataFrame(xgbc.predict_proba(X_test).T[0])\n",
    "#     test['1'] = xgbc.predict_proba(X_test).T[1]\n",
    "#     test['2'] = xgbc.predict_proba(X_test).T[2]\n",
    "#     sns.displot(test , bins=20)\n",
    "    print ( \"precision(-1, 0, 1) :\" ,precision )    \n",
    "    print ( \"recall score(-1, 0, 1) :\" ,recall )\n",
    "    print('training score :' , accuracy_score(y_train, y_train_pred))\n",
    "    print('testing score :' , accuracy_score(y_test, y_test_pred))\n",
    "    # feature important plot\n",
    "    f_importances(xgbc.feature_importances_, X_train.columns)\n",
    "\n",
    "    if isFirst :\n",
    "        predict_['date'] = trainp.date\n",
    "        isFirst = False\n",
    "        \n",
    "        predict_test['date'] = testp.date\n",
    "\n",
    "    predict_[label] = y_train_pred\n",
    "    predict_test[label] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "07d0a284",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    747\n",
       "-1    301\n",
       " 0     34\n",
       "dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f177c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train = y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c34fe",
   "metadata": {},
   "source": [
    "---\n",
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "48b32bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label :  long\n",
      "test date start form  20220111 to 20150401\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RandomOverSampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11752/1194250900.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mros\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomOverSampler' is not defined"
     ]
    }
   ],
   "source": [
    "label_ = ['long' , 'short']\n",
    "cluster = [0,1,2]\n",
    "\n",
    "predict_ = pd.DataFrame()\n",
    "isFirst = True\n",
    "for label in label_ :\n",
    "    test_ratio = 0.3\n",
    "    trainp = X [math.ceil(len(X)*test_ratio) :].reset_index(drop = True)\n",
    "    testp = X [ : math.ceil(len(X)*test_ratio)]\n",
    "    print(\"label : \", label )\n",
    "    print(\"test date start form \", testp.date[0],\"to\", testp.date[len(testp)-1] )\n",
    "\n",
    "    X_train = trainp.drop(['date'],axis = 1)\n",
    "    y_train = Y[label][math.ceil(len(X)*test_ratio) :]\n",
    "\n",
    "    X_test = testp.drop(['date'],axis = 1)\n",
    "    y_test = Y[label] [ : math.ceil(len(X)*test_ratio)]\n",
    "\n",
    "    #X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "    ros = RandomOverSampler(random_state = 40)\n",
    "\n",
    "    X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    clf=svm.SVC(kernel='rbf')\n",
    "    \n",
    "    clf.fit(X_train,y_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    precision, recall, f1, _ = score(y_test, y_test_pred)\n",
    "    print ( \"precision(-1, 0, 1:\" ,precision )    \n",
    "    print ( \"recall score(-1, 0, 1:\" ,recall )\n",
    "    Y_train_pre = clf.predict(X_train)\n",
    "    print('training score :' , accuracy_score(y_train, Y_train_pre))\n",
    "    print('testing score :' , accuracy_score(y_test, y_test_pred))\n",
    "    \n",
    "    if isFirst :\n",
    "        predict_['date'] = trainp.date\n",
    "        isFirst = False\n",
    "    predict_[label] = y_train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da9fc815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(xgb_train)):\n",
    "    if xgb_train[i] == y_test_pred[i] :\n",
    "        continue\n",
    "    else :\n",
    "        xgb_train[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1d7411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1): [0.31578947 0.2173913  0.58552632]\n",
      "recall score(-1, 0, 1): [0.30252101 0.11904762 0.68199234]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, _ = score(y_test, xgb_train)\n",
    "print ( \"precision(-1, 0, 1):\" ,precision )\n",
    "print ( \"recall score(-1, 0, 1):\" ,recall )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace261f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c861f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    304\n",
       "-1    114\n",
       " 0     46\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(xgb_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "320f0c60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>STOCK_ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA10</th>\n",
       "      <th>MA20</th>\n",
       "      <th>...</th>\n",
       "      <th>cross_6</th>\n",
       "      <th>over_3days_6</th>\n",
       "      <th>MACD_cross_9</th>\n",
       "      <th>overBuyOrSold(80/20)_9</th>\n",
       "      <th>cross_9</th>\n",
       "      <th>over_3days_9</th>\n",
       "      <th>MACD_cross_12</th>\n",
       "      <th>overBuyOrSold(80/20)_12</th>\n",
       "      <th>cross_12</th>\n",
       "      <th>over_3days_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5347</th>\n",
       "      <td>523.230771</td>\n",
       "      <td>528.102566</td>\n",
       "      <td>503.743591</td>\n",
       "      <td>503.743591</td>\n",
       "      <td>3032434.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210819</td>\n",
       "      <td>526.153845</td>\n",
       "      <td>545.348712</td>\n",
       "      <td>560.889743</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5348</th>\n",
       "      <td>516.410261</td>\n",
       "      <td>532.974364</td>\n",
       "      <td>508.615389</td>\n",
       "      <td>523.230774</td>\n",
       "      <td>2552175.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210820</td>\n",
       "      <td>525.179492</td>\n",
       "      <td>538.917944</td>\n",
       "      <td>559.769229</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5349</th>\n",
       "      <td>529.076900</td>\n",
       "      <td>542.717925</td>\n",
       "      <td>526.153823</td>\n",
       "      <td>537.846130</td>\n",
       "      <td>1603424.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210823</td>\n",
       "      <td>526.543591</td>\n",
       "      <td>535.605121</td>\n",
       "      <td>559.038458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>541.743615</td>\n",
       "      <td>543.692333</td>\n",
       "      <td>533.948743</td>\n",
       "      <td>535.897461</td>\n",
       "      <td>1928806.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210824</td>\n",
       "      <td>527.517957</td>\n",
       "      <td>532.487177</td>\n",
       "      <td>555.969229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351</th>\n",
       "      <td>544.666692</td>\n",
       "      <td>545.641051</td>\n",
       "      <td>534.923102</td>\n",
       "      <td>535.897461</td>\n",
       "      <td>1265365.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210825</td>\n",
       "      <td>527.323083</td>\n",
       "      <td>531.220514</td>\n",
       "      <td>554.166666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>536.871769</td>\n",
       "      <td>536.871769</td>\n",
       "      <td>522.256385</td>\n",
       "      <td>528.102539</td>\n",
       "      <td>1800439.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210826</td>\n",
       "      <td>532.194873</td>\n",
       "      <td>529.174359</td>\n",
       "      <td>551.389740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>521.282075</td>\n",
       "      <td>529.076947</td>\n",
       "      <td>518.358997</td>\n",
       "      <td>526.153870</td>\n",
       "      <td>1537422.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210827</td>\n",
       "      <td>532.779492</td>\n",
       "      <td>528.979492</td>\n",
       "      <td>549.051279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5354</th>\n",
       "      <td>531.025653</td>\n",
       "      <td>543.692320</td>\n",
       "      <td>528.102576</td>\n",
       "      <td>533.948730</td>\n",
       "      <td>1459895.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210830</td>\n",
       "      <td>532.000012</td>\n",
       "      <td>529.271802</td>\n",
       "      <td>547.005124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5355</th>\n",
       "      <td>533.948707</td>\n",
       "      <td>541.743579</td>\n",
       "      <td>529.076912</td>\n",
       "      <td>539.794861</td>\n",
       "      <td>1590381.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210831</td>\n",
       "      <td>532.779492</td>\n",
       "      <td>530.148724</td>\n",
       "      <td>544.764099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5356</th>\n",
       "      <td>549.538472</td>\n",
       "      <td>569.025652</td>\n",
       "      <td>542.717959</td>\n",
       "      <td>562.205139</td>\n",
       "      <td>2885272.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210901</td>\n",
       "      <td>538.041028</td>\n",
       "      <td>532.682056</td>\n",
       "      <td>543.546152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>555.384619</td>\n",
       "      <td>565.128208</td>\n",
       "      <td>551.487183</td>\n",
       "      <td>551.487183</td>\n",
       "      <td>1857591.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210902</td>\n",
       "      <td>542.717957</td>\n",
       "      <td>537.456415</td>\n",
       "      <td>541.402563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5358</th>\n",
       "      <td>555.384612</td>\n",
       "      <td>559.282048</td>\n",
       "      <td>543.692305</td>\n",
       "      <td>550.512817</td>\n",
       "      <td>1269833.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210903</td>\n",
       "      <td>547.589746</td>\n",
       "      <td>540.184619</td>\n",
       "      <td>539.551282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5359</th>\n",
       "      <td>560.256418</td>\n",
       "      <td>560.256418</td>\n",
       "      <td>538.820521</td>\n",
       "      <td>542.717957</td>\n",
       "      <td>1629320.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210906</td>\n",
       "      <td>549.343591</td>\n",
       "      <td>540.671802</td>\n",
       "      <td>538.138461</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>548.564103</td>\n",
       "      <td>548.564103</td>\n",
       "      <td>526.153846</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>1827748.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210907</td>\n",
       "      <td>547.784619</td>\n",
       "      <td>540.282056</td>\n",
       "      <td>536.384616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5361</th>\n",
       "      <td>536.000000</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>1547418.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210908</td>\n",
       "      <td>540.343591</td>\n",
       "      <td>539.192310</td>\n",
       "      <td>535.206412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5362</th>\n",
       "      <td>518.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>516.000000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>1409361.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210909</td>\n",
       "      <td>535.046155</td>\n",
       "      <td>538.882056</td>\n",
       "      <td>534.028207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>523.000000</td>\n",
       "      <td>539.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>2199352.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210910</td>\n",
       "      <td>532.343591</td>\n",
       "      <td>539.966669</td>\n",
       "      <td>534.473080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364</th>\n",
       "      <td>537.000000</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>1174271.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210913</td>\n",
       "      <td>530.600000</td>\n",
       "      <td>539.971796</td>\n",
       "      <td>534.621799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5365</th>\n",
       "      <td>536.000000</td>\n",
       "      <td>539.000000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>529.000000</td>\n",
       "      <td>1314141.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210914</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>538.892310</td>\n",
       "      <td>534.520517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5366</th>\n",
       "      <td>526.000000</td>\n",
       "      <td>533.000000</td>\n",
       "      <td>516.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>1716038.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210915</td>\n",
       "      <td>528.400000</td>\n",
       "      <td>534.371796</td>\n",
       "      <td>533.526926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>515.000000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>513.000000</td>\n",
       "      <td>518.000000</td>\n",
       "      <td>1312838.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210916</td>\n",
       "      <td>527.000000</td>\n",
       "      <td>531.023077</td>\n",
       "      <td>534.239746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5368</th>\n",
       "      <td>519.000000</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>513.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>2489748.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210917</td>\n",
       "      <td>524.400000</td>\n",
       "      <td>528.371796</td>\n",
       "      <td>534.278207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369</th>\n",
       "      <td>504.000000</td>\n",
       "      <td>505.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>488.500000</td>\n",
       "      <td>4401051.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210922</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>522.950000</td>\n",
       "      <td>531.810901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5370</th>\n",
       "      <td>492.500000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>492.500000</td>\n",
       "      <td>509.000000</td>\n",
       "      <td>2700791.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210923</td>\n",
       "      <td>511.300000</td>\n",
       "      <td>520.650000</td>\n",
       "      <td>530.466028</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5371</th>\n",
       "      <td>510.000000</td>\n",
       "      <td>529.000000</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>3329182.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210924</td>\n",
       "      <td>509.500000</td>\n",
       "      <td>518.950000</td>\n",
       "      <td>529.071155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5372</th>\n",
       "      <td>510.000000</td>\n",
       "      <td>518.000000</td>\n",
       "      <td>502.000000</td>\n",
       "      <td>513.000000</td>\n",
       "      <td>1870592.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210927</td>\n",
       "      <td>508.500000</td>\n",
       "      <td>517.750000</td>\n",
       "      <td>528.316028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>508.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>501.000000</td>\n",
       "      <td>502.000000</td>\n",
       "      <td>1012024.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210928</td>\n",
       "      <td>504.100000</td>\n",
       "      <td>514.250000</td>\n",
       "      <td>527.108334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5374</th>\n",
       "      <td>489.000000</td>\n",
       "      <td>491.500000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>3573577.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210929</td>\n",
       "      <td>503.600000</td>\n",
       "      <td>509.450000</td>\n",
       "      <td>524.710898</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>486.000000</td>\n",
       "      <td>497.500000</td>\n",
       "      <td>482.500000</td>\n",
       "      <td>495.500000</td>\n",
       "      <td>1990285.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20210930</td>\n",
       "      <td>500.900000</td>\n",
       "      <td>506.100000</td>\n",
       "      <td>522.496155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5376</th>\n",
       "      <td>485.500000</td>\n",
       "      <td>487.500000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>476.500000</td>\n",
       "      <td>2586811.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211001</td>\n",
       "      <td>494.600000</td>\n",
       "      <td>502.050000</td>\n",
       "      <td>518.210898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>480.000000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>460.500000</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>2358078.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211004</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>496.750000</td>\n",
       "      <td>513.886539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>469.000000</td>\n",
       "      <td>4588507.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211005</td>\n",
       "      <td>478.400000</td>\n",
       "      <td>491.250000</td>\n",
       "      <td>509.810898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5379</th>\n",
       "      <td>479.000000</td>\n",
       "      <td>483.500000</td>\n",
       "      <td>469.000000</td>\n",
       "      <td>474.500000</td>\n",
       "      <td>2540392.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211006</td>\n",
       "      <td>476.100000</td>\n",
       "      <td>489.850000</td>\n",
       "      <td>506.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>484.000000</td>\n",
       "      <td>497.000000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>2759235.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211007</td>\n",
       "      <td>475.400000</td>\n",
       "      <td>488.150000</td>\n",
       "      <td>504.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5381</th>\n",
       "      <td>488.000000</td>\n",
       "      <td>497.500000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>480.500000</td>\n",
       "      <td>2803566.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211008</td>\n",
       "      <td>476.200000</td>\n",
       "      <td>485.400000</td>\n",
       "      <td>502.175000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>474.000000</td>\n",
       "      <td>479.500000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>3320096.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211012</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>480.500000</td>\n",
       "      <td>499.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5383</th>\n",
       "      <td>456.500000</td>\n",
       "      <td>468.000000</td>\n",
       "      <td>447.500000</td>\n",
       "      <td>448.500000</td>\n",
       "      <td>3158230.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211013</td>\n",
       "      <td>471.900000</td>\n",
       "      <td>475.150000</td>\n",
       "      <td>494.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5384</th>\n",
       "      <td>458.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>453.000000</td>\n",
       "      <td>461.500000</td>\n",
       "      <td>2662676.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211014</td>\n",
       "      <td>469.300000</td>\n",
       "      <td>472.700000</td>\n",
       "      <td>491.075000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5385</th>\n",
       "      <td>466.500000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>469.000000</td>\n",
       "      <td>2855671.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211015</td>\n",
       "      <td>464.700000</td>\n",
       "      <td>470.050000</td>\n",
       "      <td>488.075000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5386</th>\n",
       "      <td>474.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>1660422.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211018</td>\n",
       "      <td>461.000000</td>\n",
       "      <td>468.600000</td>\n",
       "      <td>485.325000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>468.500000</td>\n",
       "      <td>481.500000</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>2366019.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211019</td>\n",
       "      <td>463.800000</td>\n",
       "      <td>469.900000</td>\n",
       "      <td>483.325000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388</th>\n",
       "      <td>480.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>474.500000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>2106073.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211020</td>\n",
       "      <td>469.100000</td>\n",
       "      <td>470.500000</td>\n",
       "      <td>480.875000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>473.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>474.000000</td>\n",
       "      <td>1905110.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211021</td>\n",
       "      <td>471.600000</td>\n",
       "      <td>470.450000</td>\n",
       "      <td>480.150000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>475.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>469.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>1325423.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211022</td>\n",
       "      <td>472.800000</td>\n",
       "      <td>468.750000</td>\n",
       "      <td>478.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5391</th>\n",
       "      <td>481.000000</td>\n",
       "      <td>495.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>3318712.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211025</td>\n",
       "      <td>477.600000</td>\n",
       "      <td>469.300000</td>\n",
       "      <td>477.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5392</th>\n",
       "      <td>486.000000</td>\n",
       "      <td>492.500000</td>\n",
       "      <td>483.500000</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>2128631.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211026</td>\n",
       "      <td>479.400000</td>\n",
       "      <td>471.600000</td>\n",
       "      <td>476.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>490.000000</td>\n",
       "      <td>502.000000</td>\n",
       "      <td>486.500000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>2148320.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211027</td>\n",
       "      <td>484.400000</td>\n",
       "      <td>476.750000</td>\n",
       "      <td>475.950000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>495.500000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>1953306.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211028</td>\n",
       "      <td>489.600000</td>\n",
       "      <td>480.600000</td>\n",
       "      <td>476.650000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>502.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>2396974.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211029</td>\n",
       "      <td>494.400000</td>\n",
       "      <td>483.600000</td>\n",
       "      <td>476.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>503.000000</td>\n",
       "      <td>509.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>2986697.0</td>\n",
       "      <td>2379</td>\n",
       "      <td>20211101</td>\n",
       "      <td>497.200000</td>\n",
       "      <td>487.400000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        High         Low       Close     Volume  STOCK_ID  \\\n",
       "5347  523.230771  528.102566  503.743591  503.743591  3032434.0      2379   \n",
       "5348  516.410261  532.974364  508.615389  523.230774  2552175.0      2379   \n",
       "5349  529.076900  542.717925  526.153823  537.846130  1603424.0      2379   \n",
       "5350  541.743615  543.692333  533.948743  535.897461  1928806.0      2379   \n",
       "5351  544.666692  545.641051  534.923102  535.897461  1265365.0      2379   \n",
       "5352  536.871769  536.871769  522.256385  528.102539  1800439.0      2379   \n",
       "5353  521.282075  529.076947  518.358997  526.153870  1537422.0      2379   \n",
       "5354  531.025653  543.692320  528.102576  533.948730  1459895.0      2379   \n",
       "5355  533.948707  541.743579  529.076912  539.794861  1590381.0      2379   \n",
       "5356  549.538472  569.025652  542.717959  562.205139  2885272.0      2379   \n",
       "5357  555.384619  565.128208  551.487183  551.487183  1857591.0      2379   \n",
       "5358  555.384612  559.282048  543.692305  550.512817  1269833.0      2379   \n",
       "5359  560.256418  560.256418  538.820521  542.717957  1629320.0      2379   \n",
       "5360  548.564103  548.564103  526.153846  532.000000  1827748.0      2379   \n",
       "5361  536.000000  541.000000  520.000000  525.000000  1547418.0      2379   \n",
       "5362  518.000000  534.000000  516.000000  525.000000  1409361.0      2379   \n",
       "5363  523.000000  539.000000  523.000000  537.000000  2199352.0      2379   \n",
       "5364  537.000000  541.000000  532.000000  534.000000  1174271.0      2379   \n",
       "5365  536.000000  539.000000  525.000000  529.000000  1314141.0      2379   \n",
       "5366  526.000000  533.000000  516.000000  517.000000  1716038.0      2379   \n",
       "5367  515.000000  525.000000  513.000000  518.000000  1312838.0      2379   \n",
       "5368  519.000000  530.000000  513.000000  524.000000  2489748.0      2379   \n",
       "5369  504.000000  505.000000  480.000000  488.500000  4401051.0      2379   \n",
       "5370  492.500000  517.000000  492.500000  509.000000  2700791.0      2379   \n",
       "5371  510.000000  529.000000  503.000000  508.000000  3329182.0      2379   \n",
       "5372  510.000000  518.000000  502.000000  513.000000  1870592.0      2379   \n",
       "5373  508.000000  512.000000  501.000000  502.000000  1012024.0      2379   \n",
       "5374  489.000000  491.500000  477.000000  486.000000  3573577.0      2379   \n",
       "5375  486.000000  497.500000  482.500000  495.500000  1990285.0      2379   \n",
       "5376  485.500000  487.500000  471.000000  476.500000  2586811.0      2379   \n",
       "5377  480.000000  482.000000  460.500000  465.000000  2358078.0      2379   \n",
       "5378  446.000000  473.000000  445.000000  469.000000  4588507.0      2379   \n",
       "5379  479.000000  483.500000  469.000000  474.500000  2540392.0      2379   \n",
       "5380  484.000000  497.000000  483.000000  492.000000  2759235.0      2379   \n",
       "5381  488.000000  497.500000  478.000000  480.500000  2803566.0      2379   \n",
       "5382  474.000000  479.500000  462.000000  464.000000  3320096.0      2379   \n",
       "5383  456.500000  468.000000  447.500000  448.500000  3158230.0      2379   \n",
       "5384  458.000000  464.000000  453.000000  461.500000  2662676.0      2379   \n",
       "5385  466.500000  476.000000  465.000000  469.000000  2855671.0      2379   \n",
       "5386  474.000000  477.000000  460.000000  462.000000  1660422.0      2379   \n",
       "5387  468.500000  481.500000  465.000000  478.000000  2366019.0      2379   \n",
       "5388  480.000000  486.000000  474.500000  475.000000  2106073.0      2379   \n",
       "5389  473.000000  485.000000  473.000000  474.000000  1905110.0      2379   \n",
       "5390  475.000000  478.000000  469.000000  475.000000  1325423.0      2379   \n",
       "5391  481.000000  495.000000  478.000000  486.000000  3318712.0      2379   \n",
       "5392  486.000000  492.500000  483.500000  487.000000  2128631.0      2379   \n",
       "5393  490.000000  502.000000  486.500000  500.000000  2148320.0      2379   \n",
       "5394  495.500000  504.000000  494.000000  500.000000  1953306.0      2379   \n",
       "5395  502.000000  504.000000  494.000000  499.000000  2396974.0      2379   \n",
       "5396  503.000000  509.000000  492.000000  500.000000  2986697.0      2379   \n",
       "\n",
       "          Date         MA5        MA10        MA20  ...  cross_6  \\\n",
       "5347  20210819  526.153845  545.348712  560.889743  ...     -1.0   \n",
       "5348  20210820  525.179492  538.917944  559.769229  ...      1.0   \n",
       "5349  20210823  526.543591  535.605121  559.038458  ...      0.0   \n",
       "5350  20210824  527.517957  532.487177  555.969229  ...      0.0   \n",
       "5351  20210825  527.323083  531.220514  554.166666  ...      0.0   \n",
       "5352  20210826  532.194873  529.174359  551.389740  ...      0.0   \n",
       "5353  20210827  532.779492  528.979492  549.051279  ...      0.0   \n",
       "5354  20210830  532.000012  529.271802  547.005124  ...      0.0   \n",
       "5355  20210831  532.779492  530.148724  544.764099  ...      0.0   \n",
       "5356  20210901  538.041028  532.682056  543.546152  ...      0.0   \n",
       "5357  20210902  542.717957  537.456415  541.402563  ...      0.0   \n",
       "5358  20210903  547.589746  540.184619  539.551282  ...      0.0   \n",
       "5359  20210906  549.343591  540.671802  538.138461  ...     -1.0   \n",
       "5360  20210907  547.784619  540.282056  536.384616  ...      0.0   \n",
       "5361  20210908  540.343591  539.192310  535.206412  ...      0.0   \n",
       "5362  20210909  535.046155  538.882056  534.028207  ...      0.0   \n",
       "5363  20210910  532.343591  539.966669  534.473080  ...      0.0   \n",
       "5364  20210913  530.600000  539.971796  534.621799  ...      0.0   \n",
       "5365  20210914  530.000000  538.892310  534.520517  ...      0.0   \n",
       "5366  20210915  528.400000  534.371796  533.526926  ...      0.0   \n",
       "5367  20210916  527.000000  531.023077  534.239746  ...      0.0   \n",
       "5368  20210917  524.400000  528.371796  534.278207  ...      0.0   \n",
       "5369  20210922  515.300000  522.950000  531.810901  ...      0.0   \n",
       "5370  20210923  511.300000  520.650000  530.466028  ...      1.0   \n",
       "5371  20210924  509.500000  518.950000  529.071155  ...      0.0   \n",
       "5372  20210927  508.500000  517.750000  528.316028  ...      0.0   \n",
       "5373  20210928  504.100000  514.250000  527.108334  ...      0.0   \n",
       "5374  20210929  503.600000  509.450000  524.710898  ...     -1.0   \n",
       "5375  20210930  500.900000  506.100000  522.496155  ...      0.0   \n",
       "5376  20211001  494.600000  502.050000  518.210898  ...      0.0   \n",
       "5377  20211004  485.000000  496.750000  513.886539  ...      0.0   \n",
       "5378  20211005  478.400000  491.250000  509.810898  ...      0.0   \n",
       "5379  20211006  476.100000  489.850000  506.400000  ...      1.0   \n",
       "5380  20211007  475.400000  488.150000  504.400000  ...      0.0   \n",
       "5381  20211008  476.200000  485.400000  502.175000  ...      0.0   \n",
       "5382  20211012  476.000000  480.500000  499.125000  ...      0.0   \n",
       "5383  20211013  471.900000  475.150000  494.700000  ...     -1.0   \n",
       "5384  20211014  469.300000  472.700000  491.075000  ...      0.0   \n",
       "5385  20211015  464.700000  470.050000  488.075000  ...      0.0   \n",
       "5386  20211018  461.000000  468.600000  485.325000  ...      0.0   \n",
       "5387  20211019  463.800000  469.900000  483.325000  ...      1.0   \n",
       "5388  20211020  469.100000  470.500000  480.875000  ...      0.0   \n",
       "5389  20211021  471.600000  470.450000  480.150000  ...      0.0   \n",
       "5390  20211022  472.800000  468.750000  478.450000  ...      0.0   \n",
       "5391  20211025  477.600000  469.300000  477.350000  ...      0.0   \n",
       "5392  20211026  479.400000  471.600000  476.050000  ...      0.0   \n",
       "5393  20211027  484.400000  476.750000  475.950000  ...      0.0   \n",
       "5394  20211028  489.600000  480.600000  476.650000  ...      0.0   \n",
       "5395  20211029  494.400000  483.600000  476.825000  ...      0.0   \n",
       "5396  20211101  497.200000  487.400000  478.000000  ...      0.0   \n",
       "\n",
       "      over_3days_6  MACD_cross_9  overBuyOrSold(80/20)_9  cross_9  \\\n",
       "5347           0.0           0.0                    -1.0      0.0   \n",
       "5348           0.0           0.0                     0.0      1.0   \n",
       "5349           0.0           0.0                     0.0      0.0   \n",
       "5350           0.0           0.0                     0.0      0.0   \n",
       "5351           0.0           0.0                     0.0      0.0   \n",
       "5352           0.0           0.0                     0.0      0.0   \n",
       "5353           0.0           0.0                     0.0      0.0   \n",
       "5354           0.0           0.0                     0.0      0.0   \n",
       "5355           0.0           0.0                     0.0      0.0   \n",
       "5356           0.0           1.0                     0.0      0.0   \n",
       "5357           0.0           0.0                     0.0      0.0   \n",
       "5358           0.0           0.0                     0.0      0.0   \n",
       "5359           0.0           0.0                     0.0     -1.0   \n",
       "5360           0.0           0.0                     0.0      0.0   \n",
       "5361           0.0          -1.0                     0.0      0.0   \n",
       "5362           0.0           0.0                     0.0      0.0   \n",
       "5363           0.0           0.0                     0.0      0.0   \n",
       "5364           0.0           0.0                     0.0      0.0   \n",
       "5365           0.0           0.0                     0.0      0.0   \n",
       "5366           0.0           0.0                     0.0      0.0   \n",
       "5367           0.0           0.0                    -1.0      0.0   \n",
       "5368           0.0           0.0                     0.0      0.0   \n",
       "5369           0.0           0.0                    -1.0      0.0   \n",
       "5370           0.0           0.0                     0.0      1.0   \n",
       "5371           0.0           0.0                     0.0      0.0   \n",
       "5372           0.0           0.0                     0.0      0.0   \n",
       "5373           0.0           0.0                     0.0      0.0   \n",
       "5374           0.0           0.0                     0.0     -1.0   \n",
       "5375           0.0           0.0                     0.0      0.0   \n",
       "5376           0.0           0.0                     0.0      0.0   \n",
       "5377           0.0           0.0                    -1.0      0.0   \n",
       "5378           0.0           0.0                     0.0      0.0   \n",
       "5379           0.0           0.0                     0.0      1.0   \n",
       "5380           0.0           0.0                     0.0      0.0   \n",
       "5381           0.0           0.0                     0.0      0.0   \n",
       "5382           0.0           0.0                     0.0      0.0   \n",
       "5383           0.0           0.0                     0.0     -1.0   \n",
       "5384           0.0           0.0                     0.0      0.0   \n",
       "5385           0.0           0.0                     0.0      1.0   \n",
       "5386           0.0           0.0                     0.0      0.0   \n",
       "5387           0.0           1.0                     0.0      0.0   \n",
       "5388           0.0           0.0                     0.0      0.0   \n",
       "5389           0.0           0.0                     0.0      0.0   \n",
       "5390           0.0           0.0                     0.0      0.0   \n",
       "5391           0.0           0.0                     0.0      0.0   \n",
       "5392           0.0           0.0                     0.0      0.0   \n",
       "5393           0.0           0.0                     0.0      0.0   \n",
       "5394           0.0           0.0                     1.0      0.0   \n",
       "5395           0.0           0.0                     1.0      0.0   \n",
       "5396           0.0           0.0                     1.0      0.0   \n",
       "\n",
       "      over_3days_9  MACD_cross_12  overBuyOrSold(80/20)_12  cross_12  \\\n",
       "5347           0.0            0.0                     -1.0       0.0   \n",
       "5348           0.0            0.0                      0.0       0.0   \n",
       "5349           0.0            0.0                      0.0       1.0   \n",
       "5350           0.0            0.0                      0.0       0.0   \n",
       "5351           0.0            0.0                      0.0       0.0   \n",
       "5352           0.0            0.0                      0.0       0.0   \n",
       "5353           0.0            0.0                      0.0       0.0   \n",
       "5354           0.0            0.0                      0.0       0.0   \n",
       "5355           0.0            0.0                      0.0       0.0   \n",
       "5356           0.0            1.0                      0.0       0.0   \n",
       "5357           0.0            0.0                      0.0       0.0   \n",
       "5358           0.0            0.0                      0.0       0.0   \n",
       "5359           0.0            0.0                      0.0       0.0   \n",
       "5360           0.0            0.0                      0.0      -1.0   \n",
       "5361           0.0           -1.0                      0.0       0.0   \n",
       "5362           0.0            0.0                      0.0       0.0   \n",
       "5363           0.0            0.0                      0.0       0.0   \n",
       "5364           0.0            0.0                      0.0       0.0   \n",
       "5365           0.0            0.0                      0.0       0.0   \n",
       "5366           0.0            0.0                      0.0       0.0   \n",
       "5367           0.0            0.0                     -1.0       0.0   \n",
       "5368           0.0            0.0                     -1.0       0.0   \n",
       "5369           0.0            0.0                     -1.0       0.0   \n",
       "5370           0.0            0.0                      0.0       1.0   \n",
       "5371           0.0            0.0                      0.0       0.0   \n",
       "5372           0.0            0.0                      0.0       0.0   \n",
       "5373           0.0            0.0                      0.0       0.0   \n",
       "5374           0.0            0.0                      0.0      -1.0   \n",
       "5375           0.0            0.0                      0.0       0.0   \n",
       "5376           0.0            0.0                      0.0       0.0   \n",
       "5377           0.0            0.0                     -1.0       0.0   \n",
       "5378           0.0            0.0                      0.0       0.0   \n",
       "5379           0.0            0.0                      0.0       1.0   \n",
       "5380           0.0            0.0                      0.0       0.0   \n",
       "5381           0.0            0.0                      0.0       0.0   \n",
       "5382           0.0            0.0                      0.0       0.0   \n",
       "5383           0.0            0.0                      0.0      -1.0   \n",
       "5384           0.0            0.0                      0.0       0.0   \n",
       "5385           0.0            0.0                      0.0       1.0   \n",
       "5386           0.0            0.0                      0.0       0.0   \n",
       "5387           0.0            1.0                      0.0       0.0   \n",
       "5388           0.0            0.0                      0.0       0.0   \n",
       "5389           0.0            0.0                      0.0       0.0   \n",
       "5390           0.0            0.0                      0.0       0.0   \n",
       "5391           0.0            0.0                      0.0       0.0   \n",
       "5392           0.0            0.0                      0.0       0.0   \n",
       "5393           0.0            0.0                      0.0       0.0   \n",
       "5394           0.0            0.0                      1.0       0.0   \n",
       "5395           0.0            0.0                      1.0       0.0   \n",
       "5396           1.0            0.0                      1.0       0.0   \n",
       "\n",
       "      over_3days_12  \n",
       "5347            0.0  \n",
       "5348            0.0  \n",
       "5349            0.0  \n",
       "5350            0.0  \n",
       "5351            0.0  \n",
       "5352            0.0  \n",
       "5353            0.0  \n",
       "5354            0.0  \n",
       "5355            0.0  \n",
       "5356            0.0  \n",
       "5357            0.0  \n",
       "5358            0.0  \n",
       "5359            0.0  \n",
       "5360            0.0  \n",
       "5361            0.0  \n",
       "5362            0.0  \n",
       "5363            0.0  \n",
       "5364            0.0  \n",
       "5365            0.0  \n",
       "5366            0.0  \n",
       "5367            0.0  \n",
       "5368            0.0  \n",
       "5369           -1.0  \n",
       "5370            0.0  \n",
       "5371            0.0  \n",
       "5372            0.0  \n",
       "5373            0.0  \n",
       "5374            0.0  \n",
       "5375            0.0  \n",
       "5376            0.0  \n",
       "5377            0.0  \n",
       "5378            0.0  \n",
       "5379            0.0  \n",
       "5380            0.0  \n",
       "5381            0.0  \n",
       "5382            0.0  \n",
       "5383            0.0  \n",
       "5384            0.0  \n",
       "5385            0.0  \n",
       "5386            0.0  \n",
       "5387            0.0  \n",
       "5388            0.0  \n",
       "5389            0.0  \n",
       "5390            0.0  \n",
       "5391            0.0  \n",
       "5392            0.0  \n",
       "5393            0.0  \n",
       "5394            0.0  \n",
       "5395            0.0  \n",
       "5396            1.0  \n",
       "\n",
       "[50 rows x 31 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAIEX_Index[-100:-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c685152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize by train\n",
    "selected = predict_[::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3c3c23dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>long</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20150401</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150408</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20150409</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150420</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150429</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>20180725</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>20180801</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>20180802</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>20180813</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>20180815</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  long  short\n",
       "0    20150401    -1      1\n",
       "1    20150408    -1     -1\n",
       "2    20150409    -1     -1\n",
       "3    20150420    -1     -1\n",
       "4    20150429    -1      1\n",
       "..        ...   ...    ...\n",
       "227  20180725     1      1\n",
       "228  20180801    -1      1\n",
       "229  20180802     1      1\n",
       "230  20180813    -1      1\n",
       "231  20180815    -1      1\n",
       "\n",
       "[232 rows x 3 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimize by test\n",
    "selected = predict_test[::-1].reset_index(drop=True)\n",
    "selected = selected [:math.ceil(len(selected)/2)].reset_index(drop=True)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c9bc6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [1,3,5,6,8,10,15]\n",
    "\n",
    "table = pd.DataFrame()\n",
    "p_table = []\n",
    "l_table = []\n",
    "d_table = []\n",
    "average = []\n",
    "\n",
    "for d in days :\n",
    "    d = d-1\n",
    "    if (d==0):\n",
    "        profit = [0.02,0.025,0.03,0.035,0.04,0.045,0.05,0.06,0.07,0.08,0.095] \n",
    "        loss = [0.01,0.015,0.02,0.025,0.035,0.045,0.055]\n",
    "        fee = 0.0015+0.0029*0.3\n",
    "    else :\n",
    "        profit = [0.03,0.05,0.075,0.1,0.125,0.15] \n",
    "        loss = [0.02,0.035,0.05,0.075,0.1,0.15,0.2,0.25]\n",
    "        fee = 0.003+0.0029*0.3\n",
    "    \n",
    "    for p in profit : \n",
    "        停利 = p\n",
    "        for l in loss :\n",
    "            停損 = l * -1\n",
    "\n",
    "            進場日期 = []\n",
    "            出場日期 = []\n",
    "            進場價格 = []\n",
    "            出場價格 = []\n",
    "            獲利 = []\n",
    "            for i in range(len(selected)):\n",
    "                if selected.short[i] == 1 :    \n",
    "\n",
    "                    temp = TAIEX_Index [TAIEX_Index.Date > selected.date[i]].reset_index(drop = True)\n",
    "                    price = temp.Open[0]\n",
    "                    進場價格.append(price)\n",
    "                    進場日期.append(temp.Date[0])\n",
    "                    flag = False\n",
    "                    for j in range(len(temp)):\n",
    "                        if temp.High[j] > price *(1.00+停利) :\n",
    "                            出場價格.append(price *(1+停利-fee))\n",
    "                            出場日期.append(temp.Date[j])\n",
    "                            break  \n",
    "                        elif temp.Low[j] < price *(1 + 停損):\n",
    "                            出場價格.append(price *(1+停損-fee))\n",
    "                            出場日期.append(temp.Date[j])\n",
    "                            break\n",
    "                        elif j > d :\n",
    "                            出場價格.append(temp.Close[j]*(1-fee))\n",
    "                            出場日期.append(temp.Date[j])\n",
    "                            break\n",
    "                    獲利.append((出場價格[-1]/進場價格[-1])-1)\n",
    "            績效表 = pd.DataFrame()\n",
    "            績效表['進場日期'] = 進場日期\n",
    "            績效表['出場日期'] = 出場日期\n",
    "            績效表['進場價格'] = 進場價格\n",
    "            績效表['出場價格'] = 出場價格\n",
    "            績效表['獲利'] = 獲利\n",
    "            \n",
    "            p_table.append(p)\n",
    "            l_table.append(l)\n",
    "            d_table.append(d)\n",
    "            average.append(sum(績效表.獲利) / (len(績效表)*(d+1)))\n",
    "            \n",
    "table['p'] = p_table\n",
    "table['l'] = l_table\n",
    "table['d'] = d_table\n",
    "table['a'] = average\n",
    "        \n",
    "#     elif selected.short[i] == 2 :    \n",
    "        \n",
    "#         temp = TAIEX_Index [TAIEX_Index.Date > selected.date[i]].reset_index(drop = True)\n",
    "#         price = temp.Open[0]\n",
    "#         進場價格.append(price)\n",
    "#         進場日期.append(temp.Date[0])\n",
    "#         flag = False\n",
    "#         for j in range(len(temp)):\n",
    "#             if temp.High[j] > price *(1.00+停利) :\n",
    "#                 出場價格.append(price *(0.995-停利))\n",
    "#                 出場日期.append(temp.Date[j])\n",
    "#                 break  \n",
    "#             elif temp.Low[j] < price *(1 + 停損):\n",
    "#                 出場價格.append(price *(0.995 - 停損))\n",
    "#                 出場日期.append(temp.Date[j])\n",
    "#                 break\n",
    "#             elif j == len(temp)-1 :\n",
    "#                 出場價格.append(temp.Close[j]*0.995)\n",
    "#                 出場日期.append(temp.Date[j])\n",
    "#                 break\n",
    "#         獲利.append((進場價格[-1]-出場價格[-1])/進場價格[-1])\n",
    "    \n",
    "\n",
    "# 持有水位 = []\n",
    "# 總獲利 = []\n",
    "# count = 0\n",
    "# profit = 0\n",
    "# test_time = date[date>selected.date[0] ]\n",
    "# for i in range(len(test_time)):\n",
    "#     profit_day = test_time[len(test_time)-1-i]\n",
    "#     buy = len( 績效表[績效表.進場日期 == profit_day])\n",
    "#     sell = len( 績效表[績效表.出場日期 == profit_day])\n",
    "#     profit += sum(績效表[績效表.出場日期 == profit_day].獲利)\n",
    "#     count += 單筆配置*(buy - sell)\n",
    "#     持有水位.append(count)\n",
    "\n",
    "# print ('統計區間 : ' + str(min(test_time)) + \" ~ \" + str(max(test_time)) )\n",
    "# print ( '最高持有金額 : ' + str(max(持有水位)))\n",
    "# print ( '總獲利 : ' + str(sum(績效表.獲利)))\n",
    "# print ( '淨利率 : ' + str(round(sum((績效表.獲利)/max(持有水位)*100) , 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0fdf7cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>l</th>\n",
       "      <th>d</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.045</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.070</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.095</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.045</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.080</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.070</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.095</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.080</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.070</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.095</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.080</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.045</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.070</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.095</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.035</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        p      l  d         a\n",
       "0   0.060  0.010  0  0.004295\n",
       "1   0.040  0.010  0  0.004273\n",
       "2   0.045  0.010  0  0.004259\n",
       "3   0.040  0.015  0  0.004224\n",
       "4   0.045  0.015  0  0.004157\n",
       "5   0.060  0.015  0  0.004135\n",
       "6   0.035  0.015  0  0.004047\n",
       "7   0.035  0.010  0  0.004003\n",
       "8   0.070  0.010  0  0.003983\n",
       "9   0.060  0.020  0  0.003880\n",
       "10  0.095  0.010  0  0.003821\n",
       "11  0.045  0.020  0  0.003821\n",
       "12  0.080  0.010  0  0.003801\n",
       "13  0.050  0.010  0  0.003799\n",
       "14  0.070  0.015  0  0.003792\n",
       "15  0.040  0.020  0  0.003670\n",
       "16  0.030  0.010  0  0.003668\n",
       "17  0.050  0.015  0  0.003639\n",
       "18  0.095  0.015  0  0.003600\n",
       "19  0.080  0.015  0  0.003580\n",
       "20  0.030  0.015  0  0.003526\n",
       "21  0.070  0.020  0  0.003508\n",
       "22  0.035  0.020  0  0.003494\n",
       "23  0.050  0.020  0  0.003384\n",
       "24  0.025  0.010  0  0.003356\n",
       "25  0.095  0.020  0  0.003335\n",
       "26  0.080  0.020  0  0.003315\n",
       "27  0.025  0.015  0  0.003265\n",
       "28  0.060  0.025  0  0.003158\n",
       "29  0.045  0.025  0  0.003098\n",
       "30  0.040  0.025  0  0.002947\n",
       "31  0.030  0.020  0  0.002942\n",
       "32  0.035  0.025  0  0.002801\n",
       "33  0.070  0.025  0  0.002754\n",
       "34  0.125  0.020  2  0.002734\n",
       "35  0.020  0.010  0  0.002671\n",
       "36  0.050  0.025  0  0.002661\n",
       "37  0.025  0.020  0  0.002612\n",
       "38  0.095  0.025  0  0.002582\n",
       "39  0.125  0.035  2  0.002579"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = table.sort_values(by=['a'],ascending=False).reset_index(drop = True)\n",
    "table[:40]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b03d5d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>l</th>\n",
       "      <th>d</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.035</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.075</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.250</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.200</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.250</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.150</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.100</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.035</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.030</td>\n",
       "      <td>0.020</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         p      l   d         a\n",
       "0    0.125  0.020   2  0.002734\n",
       "1    0.125  0.035   2  0.002579\n",
       "2    0.125  0.075   4  0.002572\n",
       "3    0.125  0.250   7  0.002556\n",
       "4    0.125  0.200   7  0.002556\n",
       "..     ...    ...  ..       ...\n",
       "283  0.030  0.250  14  0.000604\n",
       "284  0.030  0.150  14  0.000542\n",
       "285  0.030  0.100  14  0.000541\n",
       "286  0.030  0.035  14  0.000520\n",
       "287  0.030  0.020  14  0.000429\n",
       "\n",
       "[288 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swing = table[table.d > 0].reset_index(drop=True)\n",
    "swing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "976d1b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>long</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180816</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180817</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180903</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180904</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180907</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>20211210</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>20211220</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>20211224</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>20220103</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>20220111</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  long  short\n",
       "0    20180816     1      1\n",
       "1    20180817    -1     -1\n",
       "2    20180903    -1      1\n",
       "3    20180904    -1     -1\n",
       "4    20180907     1     -1\n",
       "..        ...   ...    ...\n",
       "227  20211210    -1     -1\n",
       "228  20211220    -1     -1\n",
       "229  20211224     1      1\n",
       "230  20220103    -1     -1\n",
       "231  20220111    -1      1\n",
       "\n",
       "[232 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = predict_test[::-1].reset_index(drop=True)\n",
    "selected = selected [math.ceil(len(selected)/2):].reset_index(drop=True)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8d87a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_return():\n",
    "    進場日期 = []\n",
    "    出場日期 = []\n",
    "    進場價格 = []\n",
    "    出場價格 = []\n",
    "    獲利 = []\n",
    "    for i in range(len(selected)-1):\n",
    "        if selected.short[i] == 1 :  \n",
    "            temp = TAIEX_Index [TAIEX_Index.Date > selected.date[i]].reset_index(drop = True)\n",
    "            price = temp.Open[0]\n",
    "            進場價格.append(price)\n",
    "            進場日期.append(temp.Date[0])\n",
    "            flag = False\n",
    "            for j in range(len(temp)):\n",
    "                if temp.High[j] > price *(1.00+停利) :\n",
    "                    出場價格.append(price *(return_+停利))\n",
    "                    出場日期.append(temp.Date[j])\n",
    "                    break  \n",
    "                elif temp.Low[j] < price *(1 + 停損):\n",
    "                    出場價格.append(price *(return_+停損))\n",
    "                    出場日期.append(temp.Date[j])\n",
    "                    break\n",
    "                elif j >= d :\n",
    "                    出場價格.append(temp.Close[j]*return_)\n",
    "                    出場日期.append(temp.Date[j])\n",
    "                    break\n",
    "                elif j > len(temp)-1:\n",
    "                    出場價格.append(temp.Close[j]*return_)\n",
    "                    出場日期.append(temp.Date[j])\n",
    "                    break\n",
    "            獲利.append((出場價格[-1]/進場價格[-1])-1)\n",
    "    績效表 = pd.DataFrame()\n",
    "    績效表['進場日期'] = 進場日期\n",
    "    績效表['出場日期'] = 出場日期\n",
    "    績效表['進場價格'] = 進場價格\n",
    "    績效表['出場價格'] = 出場價格\n",
    "    績效表['獲利'] = 獲利\n",
    "\n",
    "    print(績效表.獲利.std())\n",
    "    print( \"profit :{} , loss:{} , day :{} , out : {}\".format(停利,停損, d,  sum(績效表.獲利) / len(績效表)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a44d47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01734704851669195\n",
      "profit :0.06 , loss:-0.01 , day :0 , out : -0.0004357588892168283\n",
      "0.03178226740248594\n",
      "profit :0.125 , loss:-0.02 , day :2 , out : 0.0038833847145151885\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        停利 = table.p[index]\n",
    "        停損 = table.l[index]*-1\n",
    "        d=table.d[index]\n",
    "    else :\n",
    "        停利 = swing.p[index]\n",
    "        停損 = swing.l[index]*-1\n",
    "        d=swing.d[index]\n",
    "        \n",
    "    if (d==0):\n",
    "        fee = 0.0015+0.0029*0.3\n",
    "    else :\n",
    "        fee = 0.003+0.0029*0.3\n",
    "    return_ = 1-fee\n",
    "    estimate_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "07e8218e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    155\n",
       "-1     70\n",
       " 0      7\n",
       "Name: short, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected.short.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9de0842c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.559072918356123"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.0038**(240/3))**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "923742d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7704046032650247"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1+(0.0038*240/3))**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3df008b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_.to_csv('Svm_train.csv',index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3831639",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "coef_ is only available when using a linear kernel",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11184/4231588033.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mf_importances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mcoef_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcoef_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             raise AttributeError('coef_ is only available when using a '\n\u001b[0m\u001b[0;32m    502\u001b[0m                                  'linear kernel')\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: coef_ is only available when using a linear kernel"
     ]
    }
   ],
   "source": [
    "# feature important plot\n",
    "from matplotlib import pyplot as plt\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "    \n",
    "f_importances(clf.coef_[2], X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a99d40",
   "metadata": {},
   "source": [
    "## training feature : by each cluster\n",
    "- serveal index and time serises price data\n",
    "---\n",
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b356d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster :  0\n",
      "label :  long\n",
      "test date start form  20220111 to 20150515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1): [0.         0.38709677 0.2875817 ]\n",
      "recall score(-1, 0, 1): [0.         0.21556886 0.7394958 ]\n",
      "label :  short\n",
      "test date start form  20220111 to 20150515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1): [0.33333333 0.21568627 0.34911243]\n",
      "recall score(-1, 0, 1): [0.02797203 0.09821429 0.80821918]\n",
      "cluster :  1\n",
      "label :  long\n",
      "test date start form  20220111 to 20150515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1): [0.66666667 0.32352941 0.37962963]\n",
      "recall score(-1, 0, 1): [0.04444444 0.34375    0.58571429]\n",
      "label :  short\n",
      "test date start form  20220111 to 20150515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1): [0.5        0.31313131 0.32894737]\n",
      "recall score(-1, 0, 1): [0.04166667 0.52542373 0.34722222]\n",
      "cluster :  2\n",
      "label :  long\n",
      "test date start form  20220111 to 20150515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1): [0.38666667 0.43103448 0.26181818]\n",
      "recall score(-1, 0, 1): [0.12236287 0.1618123  0.73846154]\n",
      "label :  short\n",
      "test date start form  20220111 to 20150515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1): [0.34854772 0.38709677 0.30365297]\n",
      "recall score(-1, 0, 1): [0.3255814  0.09230769 0.59641256]\n",
      "cluster :  3\n",
      "label :  long\n",
      "test date start form  20220111 to 20150515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1): [0.25       0.40594059 0.3025641 ]\n",
      "recall score(-1, 0, 1): [0.03333333 0.328      0.6344086 ]\n",
      "label :  short\n",
      "test date start form  20220111 to 20150515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1): [0.2        0.43137255 0.34031414]\n",
      "recall score(-1, 0, 1): [0.03846154 0.33587786 0.65656566]\n"
     ]
    }
   ],
   "source": [
    "label_ = ['long' , 'short']\n",
    "cluster_ = [0,1,2,3]\n",
    "predict_ = pd.DataFrame()\n",
    "isFirst = True\n",
    "for cluster in cluster_ :\n",
    "    print ('cluster : ' , cluster)\n",
    "    for label in label_ :\n",
    "        test_ratio = 0.3\n",
    "        trainp = X [math.ceil(len(X)*test_ratio) :].reset_index(drop = True)\n",
    "        testp = X [ : math.ceil(len(X)*test_ratio)]\n",
    "        print(\"label : \", label )\n",
    "        print(\"test date start form \", testp.date[0],\"to\", testp.date[len(testp)-1] )\n",
    "\n",
    "        X_train = trainp.drop(['date'],axis = 1).reset_index(drop = True)\n",
    "        y_train = Y[label][math.ceil(len(X)*test_ratio) :].reset_index(drop = True)\n",
    "        \n",
    "        \n",
    "        X_test = testp.drop(['date'],axis = 1).reset_index(drop = True)\n",
    "        y_test = Y[label] [ : math.ceil(len(X)*test_ratio)].reset_index(drop = True)\n",
    "        \n",
    "        X_train_cluster = X_train[X_train.short_cluster==cluster]\n",
    "        y_train_cluster = y_train[X_train.short_cluster==cluster]\n",
    "        \n",
    "        \n",
    "        X_test_cluster =  X_test[X_test.short_cluster==cluster]\n",
    "        y_test_cluster = y_test[X_test.short_cluster==cluster]\n",
    "\n",
    "        #X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "        #ros = RandomOverSampler(random_state = 40)\n",
    "\n",
    "        #X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "        xgbc = XGBClassifier( booster='gbtree', max_depth=5,                \n",
    "                  n_estimators=200, n_jobs=4, nthread=-1, eval_metric='mlogloss',\n",
    "                  random_state=27,  tree_method='exact',\n",
    "                  validate_parameters=1, verbosity=None)\n",
    "\n",
    "        xgbc.fit(X_train_cluster,y_train_cluster)\n",
    "        y_test_predp = xgbc.predict(X_test_cluster)\n",
    "\n",
    "        precision, recall, f1, _ = score(y_test_cluster, y_test_predp)\n",
    "        print ( \"precision(-1, 0, 1):\" ,precision )\n",
    "        print ( \"recall score(-1, 0, 1):\" ,recall )\n",
    "#         if isFirst :\n",
    "#             predict_['date'] = testp.date\n",
    "#             isFirst = False\n",
    "#         predict_[label] = y_test_predp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4918297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster :  0\n",
      "label :  long\n",
      "test date start form  20220111 to 20130222\n",
      "precision(-1, 0, 1): [0.51256281 0.         0.35494881]\n",
      "recall score(-1, 0, 1): [0.46575342 0.         0.67973856]\n",
      "label :  short\n",
      "test date start form  20220111 to 20130222\n",
      "precision(-1, 0, 1): [0.49350649 0.4        0.32704403]\n",
      "recall score(-1, 0, 1): [0.38383838 0.05594406 0.68874172]\n",
      "cluster :  1\n",
      "label :  long\n",
      "test date start form  20220111 to 20130222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision(-1, 0, 1): [0.         0.29885057 0.28278689]\n",
      "recall score(-1, 0, 1): [0.         0.12935323 0.90789474]\n",
      "label :  short\n",
      "test date start form  20220111 to 20130222\n",
      "precision(-1, 0, 1): [0.         0.38912732 0.45      ]\n",
      "recall score(-1, 0, 1): [0.         0.89768977 0.23275862]\n",
      "cluster :  2\n",
      "label :  long\n",
      "test date start form  20220111 to 20130222\n",
      "precision(-1, 0, 1): [0.54255319 0.         0.28892734]\n",
      "recall score(-1, 0, 1): [0.34075724 0.         0.67611336]\n",
      "label :  short\n",
      "test date start form  20220111 to 20130222\n",
      "precision(-1, 0, 1): [0.        0.        0.3041958]\n",
      "recall score(-1, 0, 1): [0.         0.         0.98120301]\n",
      "cluster :  3\n",
      "label :  long\n",
      "test date start form  20220111 to 20130222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 16)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1352/995098132.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_cluster\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_cluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0my_test_predp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_cluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             X, y = self._validate_data(X, y, dtype=np.float64,\n\u001b[0m\u001b[0;32m    170\u001b[0m                                        \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                                        accept_large_sparse=False)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[0;32m    727\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 16)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "label_ = ['long' , 'short']\n",
    "cluster_ = [0,1,2,3]\n",
    "predict_ = pd.DataFrame()\n",
    "isFirst = True\n",
    "for cluster in cluster_ :\n",
    "    print ('cluster : ' , cluster)\n",
    "    for label in label_ :\n",
    "        test_ratio = 0.4\n",
    "        trainp = X [math.ceil(len(X)*test_ratio) :].reset_index(drop = True)\n",
    "        testp = X [ : math.ceil(len(X)*test_ratio)]\n",
    "        print(\"label : \", label )\n",
    "        print(\"test date start form \", testp.date[0],\"to\", testp.date[len(testp)-1] )\n",
    "\n",
    "        X_train = trainp.drop(['date'],axis = 1).reset_index(drop = True)\n",
    "        y_train = Y[label][math.ceil(len(X)*test_ratio) :].reset_index(drop = True)\n",
    "        \n",
    "        \n",
    "        X_test = testp.drop(['date'],axis = 1).reset_index(drop = True)\n",
    "        y_test = Y[label] [ : math.ceil(len(X)*test_ratio)].reset_index(drop = True)\n",
    "        \n",
    "        X_train_cluster = X_train[X_train.long_cluster==cluster]\n",
    "        y_train_cluster = y_train[X_train.long_cluster==cluster]\n",
    "        \n",
    "        \n",
    "        X_test_cluster =  X_test[X_test.long_cluster==cluster]\n",
    "        y_test_cluster = y_test[X_test.long_cluster==cluster]\n",
    "\n",
    "        #X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
    "        #ros = RandomOverSampler(random_state = 40)\n",
    "\n",
    "        #X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "        clf=svm.SVC(kernel='linear',C=2,gamma='auto')\n",
    "\n",
    "        clf.fit(X_train_cluster,y_train_cluster)\n",
    "        y_test_predp = clf.predict(X_test_cluster)\n",
    "\n",
    "        precision, recall, f1, _ = score(y_test_cluster, y_test_predp)\n",
    "        print ( \"precision(-1, 0, 1):\" ,precision )\n",
    "        print ( \"recall score(-1, 0, 1):\" ,recall )\n",
    "\n",
    "#         if isFirst :\n",
    "#             predict_['date'] = testp.date\n",
    "#             isFirst = False\n",
    "#         predict_[label] = y_test_predp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a1293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f95e71d4",
   "metadata": {},
   "source": [
    "day = 40\n",
    "long_list = fn.get_series_data(price_df , day , False)\n",
    "print(long_list.shape)\n",
    "\n",
    "ret = fn.triple_barrier(TAIEX_df.Close, 1.04 ,0.97, 20)\n",
    "long_label = ret.triple_barrier_signal[day-1:len(ret)]\n",
    "print('count:' ,long_label.value_counts())\n",
    "long_label = long_label[::-1].reset_index(drop=True)\n",
    "\n",
    "## CNN Training by split data\n",
    "\n",
    "model = fn.cnn_training(long_list,long_label , day , 0.5 , 50)\n",
    "\n",
    "## 取得CNN 最後一層的output\n",
    "flatten_list = []\n",
    "\n",
    "## Ini all data\n",
    "long_list = fn.get_series_data(price_df , day , False)\n",
    "long_label = ret.triple_barrier_signal[day-1:len(ret)]\n",
    "long_label = long_label[::-1].reset_index(drop=True)\n",
    "for periodData in long_list :\n",
    "    keract_inputs = periodData.reshape(1 , long_list.shape[1], long_list.shape[2],1)\n",
    "    activations = get_activations(model, keract_inputs)\n",
    "    flatten_list.append(activations['Dense'])\n",
    "\n",
    "    day = 8\n",
    "short_list = fn.get_series_data(price_df , day , True)\n",
    "print(short_list.shape)\n",
    "\n",
    "ret = fn.triple_barrier(TAIEX_df.Close, 1.01 ,0.99, 5)\n",
    "short_label = ret.triple_barrier_signal[day-1:len(ret)]\n",
    "print('count:' ,short_label.value_counts())\n",
    "short_label = short_label[::-1].reset_index(drop=True)\n",
    "\n",
    "## CNN Training\n",
    "model = cnn_training(short_list,short_label , day , 0.5 , 50)\n",
    "\n",
    "## 取得CNN 最後一層的output\n",
    "flatten_list = []\n",
    "for periodData in short_list :\n",
    "    keract_inputs = periodData.reshape(1 , short_list.shape[1], short_list.shape[2],1)\n",
    "    activations = get_activations(model, keract_inputs)\n",
    "    flatten_list.append(activations['Dense'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcddf40",
   "metadata": {},
   "source": [
    "def get_cnn_series_data( df, period) -> list:\n",
    "    list_ = []\n",
    "    for i in range(len(df)-period+1):\n",
    "        list_.append(df[i:i+period].values)\n",
    "        \n",
    "    return np.array(list_)\n",
    "\n",
    "def cnn_training(allData, allLabel, day , splitsize, epoches = 100) :\n",
    "\n",
    "    week_list = allData\n",
    "    week_label = allLabel\n",
    "    # 定義梯度下降批量\n",
    "    batch_size = 32\n",
    "    # 定義分類數量\n",
    "    num_classes = 3\n",
    "    # 定義訓練週期\n",
    "    epochs = epoches\n",
    "\n",
    "    # 定義圖像寬、高\n",
    "    img_rows, img_cols = day, 17\n",
    "    input_shape = ( img_rows, img_cols)\n",
    "\n",
    "    # 載入 MNIST 訓練資料\n",
    "    split_ratio = splitsize\n",
    "    x_train = week_list[ math.ceil(len(week_list)*split_ratio) :]\n",
    "    x_test = week_list[ : math.ceil(len(week_list)*split_ratio) ]\n",
    "\n",
    "    y_train = week_label[ math.ceil(len(week_label)*split_ratio) :]\n",
    "    y_test = week_label[ : math.ceil(len(week_label)*split_ratio) ]\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0] , img_rows, img_cols,1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols ,1)\n",
    "\n",
    "    # x_train  = torch.from_numpy(x_train)\n",
    "    # x_test  = torch.from_numpy(x_test)\n",
    "\n",
    "    # y_train = torch.from_numpy(y_train)\n",
    "    # y_test = torch.from_numpy(y_test)\n",
    "\n",
    "    input_shape = (img_rows, img_cols,1 )\n",
    "\n",
    "    # 保留原始資料，供 cross tab function 使用\n",
    "    y_test_org = y_test\n",
    "\n",
    "\n",
    "    # y 值轉成 one-hot encoding\n",
    "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "    y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    # 建立簡單的線性執行的模型\n",
    "    model = Sequential()\n",
    "    # 建立卷積層，filter=32,即 output space 的深度, Kernal Size: 3x3, activation function 採用 relu\n",
    "    model.add(Conv2D(16, kernel_size=(3,10),\n",
    "                    activation='relu',\n",
    "                    input_shape=input_shape))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(GaussianNoise(0.5))\n",
    "    # 建立卷積層，filter=64,即 output size, Kernal Size: 3x3, activation function 採用 relu\n",
    "    model.add(Conv2D(32, (3,2), activation='relu'))\n",
    "    # 建立池化層，池化大小=2x2，取最大值\n",
    "    model.add(MaxPooling2D(pool_size=(3, 5)))\n",
    "    # Dropout層隨機斷開輸入神經元，用於防止過度擬合，斷開比例:0.25\n",
    "    model.add(Dropout(0.25))\n",
    "    # Flatten層把多維的輸入一維化，常用在從卷積層到全連接層的過渡。\n",
    "    model.add(Flatten( name ='flatten'))\n",
    "    # 全連接層: 128個output\n",
    "    model.add(Dense(batch_size, 'sigmoid', name ='Dense'))\n",
    "    # 使用 softmax activation function，將結果分類\n",
    "    model.add(Dense(num_classes, activation='softmax' ))\n",
    "\n",
    "    # 編譯: 選擇損失函數、優化方法及成效衡量方式\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # 進行訓練, 訓練過程會存在 train_history 變數中\n",
    "    train_history = model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(x_test, y_test))\n",
    "\n",
    "    # 顯示損失函數、訓練成果(分數)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score)\n",
    "    print('Test accuracy:', score)\n",
    "    return model\n",
    "\n",
    "day = 10\n",
    "X_cnn = get_cnn_series_data(X.drop(['date'],axis=1),day)\n",
    "\n",
    "cnn_training(X_cnn , short_label[:len(X_cnn)], day , 0.3 , 100)\n",
    "\n",
    "\n",
    "X_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5cfbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
